import yaml
import os
import time
import traceback
import platform
import psutil
from pathlib import Path
from typing import Dict, Any, List, Optional
import docker

from src.managers.llm_api.api_manager import LLMAPIManager
from src.managers.log.logger import Logger
from src.managers.image_builder.build_image import SWEBenchLoader

# Import for SWE-bench image builder testing
from src.managers.image_builder.build_image import SWEBenchImageBuilder
from swebench.harness.utils import load_swebench_dataset, EvaluationError
from swebench.harness.test_spec.test_spec import make_test_spec
from swebench.harness.docker_build import (
    build_env_images, 
    build_instance_image, 
    setup_logger, 
    close_logger
)
from swebench.harness.docker_utils import (
    cleanup_container,
    copy_to_container,
    exec_run_with_timeout,
)
from swebench.harness.grading import get_eval_report, get_logs_eval
from swebench.harness.constants import (
    APPLY_PATCH_FAIL,
    APPLY_PATCH_PASS,
    DOCKER_PATCH,
    DOCKER_USER,
    DOCKER_WORKDIR,
    KEY_INSTANCE_ID,
    KEY_PREDICTION,
    LOG_TEST_OUTPUT,
    UTF8,
)

# Git apply commands for patch application
GIT_APPLY_CMDS = [
    "git apply --verbose",
    "git apply --verbose --reject",
    "patch --batch --fuzz=5 -p1 -i",
]


def calculate_optimal_max_workers(ram_gb_per_worker: int = 6) -> int:
    """
    Calculate optimal max_workers based on system RAM.
    
    Args:
        ram_gb_per_worker: GB of RAM required per worker (default: 6GB)
        
    Returns:
        int: Optimal number of workers, minimum 1, maximum 8
    """
    try:
        # Get system memory in bytes
        memory = psutil.virtual_memory()
        total_ram_gb = memory.total / (1024**3)  # Convert bytes to GB
        
        # Calculate max workers based on RAM
        calculated_workers = int(total_ram_gb / ram_gb_per_worker)
        
        # Apply reasonable bounds: minimum 1, maximum 8
        optimal_workers = max(1, min(calculated_workers, 8))
        
        return optimal_workers
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not detect system RAM: {e}")
        print("üîß Using default max_workers: 1")
        return 1

def format_test_results(tests_status: Dict[str, Any]) -> Dict[str, Any]:
    """
    Helper function to format test results for display.
    
    Args:
        tests_status: The tests_status dictionary from evaluation report
        
    Returns:
        Dictionary with formatted test results
    """
    f2p_success = len(tests_status.get("FAIL_TO_PASS", {}).get("success", []))
    f2p_failure = len(tests_status.get("FAIL_TO_PASS", {}).get("failure", []))
    p2p_success = len(tests_status.get("PASS_TO_PASS", {}).get("success", []))
    p2p_failure = len(tests_status.get("PASS_TO_PASS", {}).get("failure", []))
    
    return {
        "fail_to_pass": {
            "success": f2p_success,
            "failure": f2p_failure,
            "total": f2p_success + f2p_failure,
            "passed": f2p_failure == 0 and f2p_success > 0
        },
        "pass_to_pass": {
            "success": p2p_success,
            "failure": p2p_failure,
            "total": p2p_success + p2p_failure,
            "passed": p2p_failure == 0 and p2p_success > 0
        }
    }


def run_tests_on_container(
    container,
    test_spec,
    log_dir: Path,
    logger,
    timeout: int,
    test_prefix: str = ""
) -> tuple[Dict[str, Any], Optional[Dict[str, Any]], str]:
    """
    Helper function to run tests on a container and return results.
    
    Returns:
        tuple: (test_results, evaluation_report, test_output)
    """
    # Write evaluation script to container
    eval_file = log_dir / f"{test_prefix}eval.sh"
    eval_file.write_text(test_spec.eval_script, encoding=UTF8)
    copy_to_container(container, eval_file, Path("/root/eval.sh"))
    
    # Prepare the run command
    run_command = "/bin/bash /root/eval.sh"
    
    # Execute the evaluation with timeout
    test_output, timed_out, exec_time = exec_run_with_timeout(
        container, run_command, timeout
    )
    
    # Save test output to file
    test_output_file = log_dir / f"{test_prefix}{LOG_TEST_OUTPUT}"
    test_output_file.write_text(test_output, encoding=UTF8)
    
    # Parse test results
    eval_status_map, found = get_logs_eval(test_spec, str(test_output_file))
    
    test_results = None
    if not found:
        test_results = {"status": "parse_failed", "output": test_output}
    else:
        test_results = eval_status_map
    
    # Generate evaluation report
    prediction = {
        KEY_INSTANCE_ID: test_spec.instance_id,
        KEY_PREDICTION: "",  # Empty prediction for pre-patch test
    }
    
    evaluation_report = get_eval_report(
        test_spec,
        prediction,
        str(test_output_file),
        include_tests_status=True,
    )
    
    return test_results, evaluation_report, test_output


def test_image(
    image_name: str,
    instance_id: str,
    dataset_name: str = "SWE-bench/SWE-bench_Lite",
    split: str = "test",
    timeout: int = 600,
    log_dir: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Tests a patch on an existing Docker image by running tests both before and after applying the gold patch.
    """
    start_time = time.time()
    result = {
        "instance_id": instance_id,
        "image_name": image_name,
        "patch_applied": False,
        "pre_patch_tests_passed": False,
        "post_patch_tests_passed": False,
        "pre_patch_test_results": None,
        "post_patch_test_results": None,
        "pre_patch_evaluation_report": None,
        "post_patch_evaluation_report": None,
        "pre_patch_terminal_output": None,
        "post_patch_terminal_output": None,
        "error": None,
        "logs": None,
    }
    
    try:
        # Load the specific instance from the dataset to get the gold patch
        dataset = load_swebench_dataset(dataset_name, split, [instance_id])
        
        if not dataset:
            raise ValueError(f"Instance {instance_id} not found in dataset {dataset_name}")
        
        instance = dataset[0]
        
        # Create test spec from the instance
        test_spec = make_test_spec(instance)
        
        # Set up Docker client
        client = docker.from_env()
        
        # Set up log directory
        if log_dir is None:
            log_dir = Path.cwd() / "logs" / "test_image" / instance_id
        else:
            log_dir = Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        result["logs"] = str(log_dir)
        
        # Set up logger
        log_file = log_dir / "test_image.log"
        logger = setup_logger(instance_id, log_file)
        
        try:
            # Check if image exists
            try:
                client.images.get(image_name)
            except docker.errors.ImageNotFound:
                raise ValueError(f"Image {image_name} not found")
            
            # Create container from the image
            container_name = f"test_{instance_id}_{int(time.time())}"
            
            # Get run args and platform from test_spec
            run_args = test_spec.docker_specs.get("run_args", {})
            cap_add = run_args.get("cap_add", [])
            
            container = client.containers.create(
                image_name,
                name=container_name,
                working_dir=DOCKER_WORKDIR,
                user=DOCKER_USER,
                detach=True,
                command="tail -f /dev/null",  # Keep container running
                platform=test_spec.platform,  # Ensure correct architecture
                cap_add=cap_add,  # Add capabilities if needed
            )
            
            try:
                # Start the container
                container.start()
                
                # Run tests BEFORE applying the patch
                pre_patch_test_results, pre_patch_evaluation_report, pre_patch_output = run_tests_on_container(
                    container, test_spec, log_dir, logger, timeout, "pre_patch_"
                )
                
                result["pre_patch_test_results"] = pre_patch_test_results
                result["pre_patch_evaluation_report"] = pre_patch_evaluation_report
                result["pre_patch_terminal_output"] = pre_patch_output
                
                # Check if pre-patch tests passed and show results
                if pre_patch_evaluation_report:
                    instance_report = pre_patch_evaluation_report.get(instance_id, {})
                    result["pre_patch_tests_passed"] = instance_report.get("resolved", False)
                    
                    # Show detailed results
                    if "tests_status" in instance_report:
                        tests_status = instance_report["tests_status"]
                        formatted_results = format_test_results(tests_status)
                        result["pre_patch_test_details"] = formatted_results
                        
                        f2p = formatted_results["fail_to_pass"]
                        p2p = formatted_results["pass_to_pass"]
                        
                        print(f"Pre-patch test results:")
                        print(f"  FAIL_TO_PASS: {f2p['success']}/{f2p['total']} passed {'‚úÖ' if f2p['passed'] else '‚ùå'}")
                        print(f"  PASS_TO_PASS: {p2p['success']}/{p2p['total']} passed {'‚úÖ' if p2p['passed'] else '‚ùå'}")
                    else:
                        result["pre_patch_test_details"] = None
                        print("Pre-patch: Could not determine detailed test results")
                else:
                    result["pre_patch_test_details"] = None
                    print("Pre-patch: Could not determine test results")
                
                # Get the gold patch from the instance
                gold_patch = instance.get("patch", "")
                if not gold_patch:
                    raise ValueError(f"No gold patch found for instance {instance_id}")
                
                # Write patch to file and copy to container
                patch_file = log_dir / "gold_patch.diff"
                patch_file.write_text(gold_patch, encoding=UTF8)
                logger.info(f"Gold patch written to {patch_file}")
                
                copy_to_container(container, patch_file, Path(DOCKER_PATCH))
                logger.info(f"Gold patch copied to container at {DOCKER_PATCH}")
                
                # Apply the gold patch to the container
                print("Applying gold patch to container...")
                applied_patch = False
                for git_apply_cmd in GIT_APPLY_CMDS:
                    try:
                        exit_code, output = container.exec_run(
                            f"{git_apply_cmd} {DOCKER_PATCH}",
                            workdir=DOCKER_WORKDIR,
                            user=DOCKER_USER,
                        )
                        if exit_code == 0:
                            logger.info(f"{APPLY_PATCH_PASS}:\n{output.decode(UTF8)}")
                            applied_patch = True
                            break
                        else:
                            logger.info(f"Failed to apply patch with {git_apply_cmd}: {output.decode(UTF8)}")
                    except Exception as e:
                        logger.info(f"Error applying patch with {git_apply_cmd}: {str(e)}")
                        continue
                
                if not applied_patch:
                    error_msg = f"{APPLY_PATCH_FAIL}: Could not apply gold patch with any method"
                    logger.error(error_msg)
                    raise EvaluationError(instance_id, error_msg, logger)
                
                result["patch_applied"] = True
                print("‚úÖ Gold patch applied successfully")
                
                # Get git diff to see what changed
                _, git_diff_bytes = container.exec_run(
                    "git -c core.fileMode=false diff", workdir=DOCKER_WORKDIR
                )
                git_diff_output = git_diff_bytes.decode(UTF8).strip()
                logger.info(f"Git diff after patch application:\n{git_diff_output}")
                
                # Run tests AFTER applying the patch
                print("Running tests after applying patch...")
                post_patch_test_results, post_patch_evaluation_report, post_patch_output = run_tests_on_container(
                    container, test_spec, log_dir, logger, timeout, "post_patch_"
                )
                
                result["post_patch_test_results"] = post_patch_test_results
                result["post_patch_evaluation_report"] = post_patch_evaluation_report
                result["post_patch_terminal_output"] = post_patch_output
                
                # Check if post-patch tests passed
                if post_patch_evaluation_report:
                    instance_report = post_patch_evaluation_report.get(instance_id, {})
                    result["post_patch_tests_passed"] = instance_report.get("resolved", False)
                    
                    # Show detailed results
                    if "tests_status" in instance_report:
                        tests_status = instance_report["tests_status"]
                        formatted_results = format_test_results(tests_status)
                        result["post_patch_test_details"] = formatted_results
                        
                        f2p = formatted_results["fail_to_pass"]
                        p2p = formatted_results["pass_to_pass"]
                        
                        print(f"Post-patch test results:")
                        print(f"  FAIL_TO_PASS: {f2p['success']}/{f2p['total']} passed {'‚úÖ' if f2p['passed'] else '‚ùå'}")
                        print(f"  PASS_TO_PASS: {p2p['success']}/{p2p['total']} passed {'‚úÖ' if p2p['passed'] else '‚ùå'}")
                        
                        if result["post_patch_tests_passed"]:
                            print("‚úÖ All post-patch tests passed!")
                        else:
                            print("‚ùå Some post-patch tests failed")
                    else:
                        result["post_patch_test_details"] = None
                        print("Post-patch: Could not determine detailed test results")
                else:
                    result["post_patch_test_details"] = None
                    print("‚ö†Ô∏è  Could not determine post-patch test results")
                
            finally:
                # Clean up container
                cleanup_container(client, container, logger)
                
        finally:
            close_logger(logger)
            
    except Exception as e:
        error_msg = f"Error testing image {image_name} with instance {instance_id}: {str(e)}"
        print(f"‚ùå {error_msg}")
        result["error"] = error_msg
        print(f"Traceback: {traceback.format_exc()}")
    
    return result


def test_swe_image_builder(
    instance_ids: List[str],
    dataset_name: str = "SWE-bench/SWE-bench_Lite",
    split: str = "test",
    max_workers: int = 2,
    force_rebuild: bool = False,
    timeout: int = 600,
) -> Dict[str, Any]:
    """
    Test the SWEBenchImageBuilder by building images for multiple instances and running evaluations.
    
    This function provides a comprehensive testing framework for the SWEBenchImageBuilder class.
    It performs the following operations:
    
    1. **Image Building**: Uses SWEBenchImageBuilder to build Docker images for specified SWE-bench instances
    2. **Pre-patch Testing**: Runs tests on the built images before applying any patches
    3. **Patch Application**: Applies the gold patch from the SWE-bench dataset to the container
    4. **Post-patch Testing**: Runs tests again after patch application to verify the fix
    5. **Result Analysis**: Compares pre-patch and post-patch test results to determine success
    
    Usage Examples:
    
    # Basic usage with default parameters
    results = test_swe_image_builder(["django__django-10914"])
    
    # Test multiple instances with custom settings
    results = test_swe_image_builder(
        instance_ids=["django__django-10914", "astropy__astropy-12907"],
        dataset_name="SWE-bench/SWE-bench",
        split="test",
        max_workers=1,
        force_rebuild=True,
        timeout=900
    )
    
    # Test with SWE-bench Lite dataset
    results = test_swe_image_builder(
        instance_ids=["django__django-10914"],
        dataset_name="SWE-bench/SWE-bench_Lite",
        split="dev"
    )
    
    Args:
        instance_ids (List[str]): List of SWE-bench instance IDs to build and test.
                                 Format: "repository__repository-issue_number"
                                 Example: ["django__django-10914", "astropy__astropy-12907"]
        
        dataset_name (str, optional): Name of the SWE-bench dataset to use.
                                    Defaults to "SWE-bench/SWE-bench_Lite".
                                    Options: "SWE-bench/SWE-bench", "SWE-bench/SWE-bench_Lite"
        
        split (str, optional): Dataset split to use. Defaults to "test".
                              Options: "dev", "test"
        
        max_workers (int, optional): Number of parallel workers for image building.
                                   Defaults to 2. Use 1 for sequential building to avoid memory issues.
        
        force_rebuild (bool, optional): Whether to force rebuild existing images.
                                      Defaults to False. Set to True to rebuild even if images exist.
        
        timeout (int, optional): Timeout in seconds for test execution. Defaults to 600 (10 minutes).
    
    Returns:
        Dict[str, Any]: Comprehensive test results containing:
            - instance_results: Dictionary mapping instance_id to detailed results
            - errors: List of error messages encountered during testing
            
        Each instance result contains:
            - image_name: Name of the built Docker image
            - build_status: Status of the image build process
            - test_result: Detailed test results including:
                - patch_applied: Whether the gold patch was successfully applied
                - pre_patch_tests_passed: Whether tests passed before patch application
                - post_patch_tests_passed: Whether tests passed after patch application
                - pre_patch_test_details: Detailed breakdown of pre-patch test results
                - post_patch_test_details: Detailed breakdown of post-patch test results
                - logs: Path to log files for debugging
    
    Test Result Interpretation:
        - ‚úÖ PERFECT: Pre-patch tests failed, post-patch tests passed (patch fixed the issue)
        - ‚úÖ GOOD: All tests pass both before and after patch (no regression)
        - ‚ö†Ô∏è  WARNING: Pre-patch tests passed, post-patch tests failed (patch broke something)
        - ‚ùå ISSUE: Tests fail both before and after patch (patch didn't fix the issue)
    
    Requirements:
        - Docker must be installed and running
        - SWE-bench package must be installed
        - Sufficient disk space for Docker images (several GB per instance)
        - Network access to download datasets and base images
    
    Error Handling:
        - Gracefully handles Docker image build failures
        - Continues testing other instances if one fails
        - Provides detailed error messages and stack traces
        - Logs all operations for debugging purposes
    
    Performance Notes:
        - Image building can take 10-30 minutes per instance
        - Test execution typically takes 5-15 minutes per instance
        - Use max_workers=1 for memory-constrained environments
        - Consider using SWE-bench_Lite for faster testing with smaller instances
    """
    start_time = time.time()
    results = {
        "instance_results": {},
        "errors": []
    }
    
    try:
        print(f"üöÄ Starting SWEBenchImageBuilder test with {len(instance_ids)} instances")
        print(f"Instance IDs: {instance_ids}")
        
        # Step 1: Build images using SWEBenchImageBuilder
        print("\nüì¶ Step 1: Building images with SWEBenchImageBuilder...")
        builder = SWEBenchImageBuilder(
            dataset_name=dataset_name,
            split=split,
            instance_ids=instance_ids,
            max_workers=max_workers,
            force_rebuild=force_rebuild,
        )
        
        # Step 2: Test each successfully built image
        print("\nüß™ Step 2: Testing built images...")
        
        for instance_id in instance_ids:
            print(f"\n--- Testing instance: {instance_id} ---")
            
            try:
                # Get image name from builder
                image_name = builder.get_image_name(instance_id)
                build_status = builder.get_build_status(instance_id)
                
                print(f"Image name: {image_name}")
                print(f"Build status: {build_status}")
                
                if build_status in ['successful', 'already_exists']:
                    # Test the image
                    test_result = test_image(
                        image_name=image_name,
                        instance_id=instance_id,
                        dataset_name=dataset_name,
                        split=split,
                        timeout=timeout,
                    )
                    
                    results["instance_results"][instance_id] = {
                        "image_name": image_name,
                        "build_status": build_status,
                        "test_result": test_result,
                        "success": test_result.get("patch_applied", False)
                    }
                    
                    if test_result.get("patch_applied", False):
                        print(f"‚úÖ {instance_id}: Image built and patch applied successfully")
                        
                        # Show test results summary
                        pre_patch_passed = test_result.get('pre_patch_tests_passed', False)
                        post_patch_passed = test_result.get('post_patch_tests_passed', False)
                        pre_patch_details = test_result.get('pre_patch_test_details')
                        post_patch_details = test_result.get('post_patch_test_details')
                        
                        print(f"  Pre-patch tests: {'‚úÖ PASSED' if pre_patch_passed else '‚ùå FAILED'}")
                        if pre_patch_details:
                            f2p = pre_patch_details["fail_to_pass"]
                            p2p = pre_patch_details["pass_to_pass"]
                            print(f"    FAIL_TO_PASS: {f2p['success']}/{f2p['total']} {'‚úÖ' if f2p['passed'] else '‚ùå'}")
                            print(f"    PASS_TO_PASS: {p2p['success']}/{p2p['total']} {'‚úÖ' if p2p['passed'] else '‚ùå'}")
                        
                        print(f"  Post-patch tests: {'‚úÖ PASSED' if post_patch_passed else '‚ùå FAILED'}")
                        if post_patch_details:
                            f2p = post_patch_details["fail_to_pass"]
                            p2p = post_patch_details["pass_to_pass"]
                            print(f"    FAIL_TO_PASS: {f2p['success']}/{f2p['total']} {'‚úÖ' if f2p['passed'] else '‚ùå'}")
                            print(f"    PASS_TO_PASS: {p2p['success']}/{p2p['total']} {'‚úÖ' if p2p['passed'] else '‚ùå'}")
                        
                        if not pre_patch_passed and post_patch_passed:
                            print("  üéØ PERFECT: Patch fixed failing tests!")
                        elif pre_patch_passed and post_patch_passed:
                            print("  ‚úÖ GOOD: All tests pass both before and after patch")
                        elif pre_patch_passed and not post_patch_passed:
                            print("  ‚ö†Ô∏è  WARNING: Patch broke previously passing tests")
                        else:
                            print("  ‚ùå ISSUE: Tests fail both before and after patch")
                    else:
                        print(f"‚ùå {instance_id}: Patch could not be applied")
                        results["errors"].append(f"{instance_id}: Patch application failed")
                        
                else:
                    print(f"‚ùå {instance_id}: Image build failed (status: {build_status})")
                    results["instance_results"][instance_id] = {
                        "image_name": None,
                        "build_status": build_status,
                        "test_result": None,
                        "success": False
                    }
                    results["errors"].append(f"{instance_id}: Image build failed")
                    
            except Exception as e:
                error_msg = f"Error testing instance {instance_id}: {str(e)}"
                print(f"‚ùå {error_msg}")
                results["errors"].append(error_msg)
                results["instance_results"][instance_id] = {
                    "image_name": None,
                    "build_status": "error",
                    "test_result": None,
                    "success": False,
                    "error": error_msg
                }
        
    except Exception as e:
        error_msg = f"Error in test_swe_image_builder: {str(e)}"
        print(f"‚ùå {error_msg}")
        results["errors"].append(error_msg)
        print(f"Traceback: {traceback.format_exc()}")
    
    return results


def load_config(config_path: str = "src/config/config.yaml") -> dict:
    """Âä†ËΩΩ YAML ÈÖçÁΩÆÊñá‰ª∂"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"ÈÖçÁΩÆÊñá‰ª∂‰∏çÂ≠òÂú®: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

def load_swe_bench_data(config: dict, logger):
    """Âä†ËΩΩÂíåÂ§ÑÁêÜ SWE-bench Êï∞ÊçÆÈõÜ"""
    dataset_config = config.get("dataset", {})
    workspace_config = config.get("workspace", {})
    
    dataset_name = dataset_config.get("name", "princeton-nlp/SWE-bench_Lite")
    split_name = dataset_config.get("split", "dev")
    workspace_path = workspace_config.get("path", "workspace")
    
    logger.info(f"ÂºÄÂßãÂ§ÑÁêÜ SWE-bench Êï∞ÊçÆÈõÜ: {dataset_name}")
    
    # ÂàõÂª∫Êï∞ÊçÆÂä†ËΩΩÂô®
    swe_loader = SWEBenchLoader(
        dataset_name=dataset_name,
        split_name=split_name,
        workspace_path=workspace_path,
        logger=logger
    )
    
    # Âä†ËΩΩÂπ∂Â§ÑÁêÜÊï∞ÊçÆÈõÜÔºàÈôêÂà∂Â§ÑÁêÜÊï∞ÈáèÁî®‰∫éÊµãËØïÔºâ
    max_items = int(os.getenv("MAX_ITEMS", "5"))  # ÈªòËÆ§Â§ÑÁêÜ5‰∏™ÔºåÂèØÈÄöËøáÁéØÂ¢ÉÂèòÈáèË∞ÉÊï¥
    result = swe_loader.load_and_process_all(max_items=max_items)
    
    # ÊòæÁ§∫ÁªüËÆ°‰ø°ÊÅØ
    stats = swe_loader.get_stats()
    logger.info(f"Êï∞ÊçÆÂ§ÑÁêÜÁªüËÆ°: {stats}")
    
    print(f"\nüìä SWE-bench Êï∞ÊçÆÂ§ÑÁêÜÂÆåÊàê:")
    print(f"   Êï∞ÊçÆÈõÜ: {dataset_name}")
    print(f"   ÂàÜÂâ≤: {split_name}")
    print(f"   Â∑•‰ΩúÁ©∫Èó¥: {workspace_path}")
    print(f"   Â§ÑÁêÜÁªìÊûú: {result}")
    print(f"   ÁªüËÆ°‰ø°ÊÅØ: {stats}")
    
    return swe_loader, result

def test_embedding_functionality(config: dict, logger):
    """ÊµãËØï embedding ÂäüËÉΩ"""
    rag_config = config.get("rag", {})
    embedding_config = rag_config.get("embedding", {})
    
    # Ê£ÄÊü•ÊòØÂê¶ÂêØÁî® embedding
    if not embedding_config.get("enabled", False):
        logger.info("Embedding ÂäüËÉΩÊú™ÂêØÁî®ÔºåË∑≥ËøáÊµãËØï")
        print("‚è≠Ô∏è  Embedding ÂäüËÉΩÊú™ÂêØÁî®ÔºåË∑≥ËøáÊµãËØï")
        return
    
    client_name = embedding_config.get("client", "openai")
    model_name = embedding_config.get("model", "text-embedding-3-small")
    
    logger.info(f"ÂºÄÂßãÊµãËØï Embedding ÂäüËÉΩ - ÂÆ¢Êà∑Á´Ø: {client_name}, Ê®°Âûã: {model_name}")
    
    print("üß† RAG Embedding ÂäüËÉΩÊµãËØï")
    print("=" * 60)
    print(f"üîß ÈÖçÁΩÆ: ÂÆ¢Êà∑Á´Ø={client_name}, Ê®°Âûã={model_name}")
    print("=" * 60)
    
    # ÊµãËØïÊñáÊú¨
    test_texts = [
        "‰∫∫Â∑•Êô∫ËÉΩÊòØËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÁöÑ‰∏Ä‰∏™ÂàÜÊîØÔºåËá¥Âäõ‰∫éÂàõÂª∫ËÉΩÂ§üÊ®°Êãü‰∫∫Á±ªÊô∫ËÉΩÁöÑÁ≥ªÁªü„ÄÇ",
        "Machine learning algorithms can automatically learn and improve from experience.",
        "Ê∑±Â∫¶Â≠¶‰π†‰ΩøÁî®Â§öÂ±ÇÁ•ûÁªèÁΩëÁªúÊù•Â§ÑÁêÜÂíåÂàÜÊûêÂ§çÊùÇÁöÑÊï∞ÊçÆÊ®°Âºè„ÄÇ",
        "Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∏ÆÂä©ËÆ°ÁÆóÊú∫ÁêÜËß£ÂíåÁîüÊàê‰∫∫Á±ªËØ≠Ë®Ä„ÄÇ"
    ]
    
    print(f"üìù ÊµãËØïÊñáÊú¨ ({len(test_texts)} Êù°):")
    for i, text in enumerate(test_texts, 1):
        print(f"   {i}. {text}")
        logger.debug(f"ÊµãËØïÊñáÊú¨ {i}: {text}")
    
    try:
        # ‰ΩøÁî® LLMAPIManager ÂàõÂª∫Áªü‰∏ÄÁöÑÂÆ¢Êà∑Á´ØÁÆ°ÁêÜÂô®
        manager = LLMAPIManager(
            client_name=client_name,
            timeout=30,
            max_retries=2,
            logger=logger
        )
        
        logger.info(f"ÊàêÂäüÂàõÂª∫ {client_name.upper()} ÂÆ¢Êà∑Á´ØÁÆ°ÁêÜÂô®")
        print(f"‚úÖ ÊàêÂäüÂàõÂª∫ {client_name.upper()} ÂÆ¢Êà∑Á´ØÁÆ°ÁêÜÂô®")
        
        print(f"\nüöÄ ÂºÄÂßãÁîüÊàêÂµåÂÖ•ÂêëÈáè...")
        logger.info(f"ÂºÄÂßãË∞ÉÁî® embedding API - Ê®°Âûã: {model_name}")
        
        # ÈÄöËøá LLMAPIManager Ë∞ÉÁî® embedding APIÔºàÊîØÊåÅÊâπÈáèÂ§ÑÁêÜÔºâ
        response = manager.create_embeddings(
            input_text=test_texts,  # Áõ¥Êé•‰º†ÈÄíÊñáÊú¨ÂàóË°®ÔºåÊîØÊåÅÊâπÈáèÂ§ÑÁêÜ
            model=model_name,
            timeout=30,
            retry=2
        )
        
        # Ê£ÄÊü•ÂìçÂ∫îÊòØÂê¶ÊàêÂäü
        if response is None:
            error_msg = "Embedding ÁîüÊàêÂ§±Ë¥•: ÊâÄÊúâÈáçËØïÈÉΩÂ§±Ë¥•"
            logger.error(error_msg)
            print(f"‚ùå {error_msg}")
            return
        
        # ËÆ∞ÂΩïÊàêÂäüÁªìÊûú
        logger.info(f"Embedding ÁîüÊàêÊàêÂäü - Ê®°Âûã: {response.model}, ÂêëÈáèÊï∞Èáè: {len(response.data)}, Token‰ΩøÁî®: {response.usage.total_tokens}")
        
        print("‚úÖ ÂµåÂÖ•ÂêëÈáèÁîüÊàêÊàêÂäü!")
        print(f"\nüìä ÂìçÂ∫îÁªüËÆ°:")
        print(f"   ü§ñ ‰ΩøÁî®Ê®°Âûã: {response.model}")
        print(f"   üìà ÂµåÂÖ•ÂêëÈáèÊï∞Èáè: {len(response.data)}")
        print(f"   üî¢ Token ‰ΩøÁî®: {response.usage.prompt_tokens} prompt + {response.usage.total_tokens} total")
        
        # ÊòæÁ§∫ÊØè‰∏™ÂµåÂÖ•ÂêëÈáèÁöÑËØ¶ÁªÜ‰ø°ÊÅØ
        print(f"\nüîç ÂµåÂÖ•ÂêëÈáèËØ¶ÊÉÖ:")
        total_dimensions = 0
        for i, embedding_data in enumerate(response.data):
            vector_dim = len(embedding_data.embedding)
            total_dimensions += vector_dim
            first_few = embedding_data.embedding[:3]  # ÊòæÁ§∫Ââç3‰∏™ÂÄº
            last_few = embedding_data.embedding[-3:]  # ÊòæÁ§∫Âêé3‰∏™ÂÄº
            
            print(f"   ÂêëÈáè {i+1}: Áª¥Â∫¶={vector_dim}")
            print(f"           Ââç3‰∏™ÂÄº: {[round(x, 6) for x in first_few]}")
            print(f"           Âêé3‰∏™ÂÄº: {[round(x, 6) for x in last_few]}")
            
            # ËÆ∞ÂΩïÂà∞Êó•Âøó
            logger.debug(f"ÂêëÈáè {i+1}: Áª¥Â∫¶={vector_dim}, Á¥¢Âºï={embedding_data.index}")
        
        avg_dimension = total_dimensions // len(response.data) if response.data else 0
        logger.info(f"Âπ≥ÂùáÂêëÈáèÁª¥Â∫¶: {avg_dimension}")
        
        print(f"\nüéØ ÊµãËØïÊÄªÁªì:")
        print(f"   ‚úÖ ÊàêÂäüÁîüÊàê {len(response.data)} ‰∏™ÂµåÂÖ•ÂêëÈáè")
        print(f"   üìè Âπ≥ÂùáÂêëÈáèÁª¥Â∫¶: {avg_dimension}")
        print(f"   ‚ö° Token ÊïàÁéá: {response.usage.total_tokens / len(test_texts):.1f} tokens/text")
        print(f"   üéâ Embedding ÂäüËÉΩÊµãËØïÂÆåÊàê!")
        
        logger.info("Embedding ÂäüËÉΩÊµãËØïÊàêÂäüÂÆåÊàê")
        
        # ÂÖ≥Èó≠ÁÆ°ÁêÜÂô®
        manager.close()
        
    except Exception as e:
        error_msg = f"Embedding ÊµãËØïÂ§±Ë¥•: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå {error_msg}")
        
        # Êèê‰æõÊïÖÈöúÊéíÈô§Âª∫ËÆÆ
        print(f"\nüí° ÊïÖÈöúÊéíÈô§Âª∫ËÆÆ:")
        print(f"   1. Ê£ÄÊü• API ÂØÜÈí•ÊòØÂê¶Ê≠£Á°ÆËÆæÁΩÆ")
        print(f"   2. Á°ÆËÆ§ÁΩëÁªúËøûÊé•Ê≠£Â∏∏")
        print(f"   3. È™åËØÅÊ®°ÂûãÂêçÁß∞ÊòØÂê¶Ê≠£Á°Æ: {model_name}")
        print(f"   4. Ê£ÄÊü• API ÈÖçÈ¢ùÊòØÂê¶ÂÖÖË∂≥")

def test_provider_models(config: dict, logger):
    """ÊµãËØïÊâÄÊúâÈÖçÁΩÆÁöÑÊèê‰æõÂïÜÂíåÊ®°Âûã"""
    providers = config.get("providers", {})
    
    # Áõ¥Êé•Âú®‰ª£Á†Å‰∏≠ÂÆö‰πâÊµãËØïÂèÇÊï∞
    test_message = "ÂõõÂ§ßÂêçËëóÊúâÂì™‰∫õÔºüËØ∑ÁÆÄË¶Å‰ªãÁªçÊØè‰∏ÄÈÉ®„ÄÇ"
    system_message = "‰Ω†ÊòØ‰∏Ä‰∏™ÊúâÁî®ÁöÑAIÂä©ÊâãÔºåËØ∑Áî®ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÊñπÂºèÂõûÁ≠îÈóÆÈ¢ò„ÄÇ"
    temperature = 0.1
    max_tokens = 500
    stream = False
    timeout = 30
    
    print("üöÄ LLM API Â§öÊèê‰æõÂïÜÊ®°ÂûãÊµãËØï")
    print("=" * 80)
    print(f"üìù ÊµãËØïÊ∂àÊÅØ: {test_message}")
    print(f"üîß ÈÖçÁΩÆ: stream={stream}, temperature={temperature}, max_tokens={max_tokens}")
    print("=" * 80)
    
    total_tests = 0
    successful_tests = 0
    
    # ÈÅçÂéÜÊØè‰∏™Êèê‰æõÂïÜ
    for provider_name, models in providers.items():
        logger.info(f"ÂºÄÂßãÊµãËØïÊèê‰æõÂïÜ: {provider_name}")
        print(f"\nüè¢ ÊµãËØïÊèê‰æõÂïÜ: {provider_name.upper()}")
        print("-" * 60)
        
        try:
            # ÂàõÂª∫ËØ•Êèê‰æõÂïÜÁöÑÁÆ°ÁêÜÂô®
            manager = LLMAPIManager(
                client_name=provider_name,
                stream=stream,
                timeout=timeout,
                logger=logger
            )
            
            logger.info(f"{provider_name} ÂÆ¢Êà∑Á´ØÂàõÂª∫ÊàêÂäü")
            print(f"‚úÖ {provider_name} ÂÆ¢Êà∑Á´ØÂàõÂª∫ÊàêÂäü")
            
            if provider_name == "private":
                model_name = os.getenv("PRIVATE_MODEL_NAME", "qwen-2.5-coder-3b-instruct")
                total_tests += 1
                response = manager.chat(
                    model=model_name,
                    message=test_message,
                    system_message=system_message,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                if response is not None:
                    logger.info(f"Ê®°Âûã {model_name} ÊµãËØïÊàêÂäüÔºåÂìçÂ∫îÈïøÂ∫¶: {len(response)} Â≠óÁ¨¶")
                    print(f"üì§ ËØ∑Ê±ÇÊàêÂäü")
                    print(f"üì• ÂìçÂ∫îÂÜÖÂÆπ:")
                    print(f"   {response}")
                    print(f"‚úÖ Ê®°Âûã {model_name} ÊµãËØïÊàêÂäü")
                    successful_tests += 1
                else:
                    logger.warning(f"Ê®°Âûã {model_name} ÊµãËØïÂ§±Ë¥•: ÊâÄÊúâÈáçËØïÈÉΩÂ§±Ë¥•")
                    print(f"‚ùå Ê®°Âûã {model_name} ÊµãËØïÂ§±Ë¥•: ÊâÄÊúâÈáçËØïÈÉΩÂ§±Ë¥•ÔºåËøîÂõû None")
            else:
                # ÈÅçÂéÜËØ•Êèê‰æõÂïÜÁöÑÊâÄÊúâÊ®°Âûã
                for model_name in models:
                    total_tests += 1
                    print(f"\nü§ñ ÊµãËØïÊ®°Âûã: {model_name}")
                    print("." * 40)
                    
                    # Ë∞ÉÁî®ËÅäÂ§©Êé•Âè£
                    response = manager.chat(
                        model=model_name,
                        message=test_message,
                        system_message=system_message,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    if response is not None:
                        logger.info(f"Ê®°Âûã {model_name} ÊµãËØïÊàêÂäüÔºåÂìçÂ∫îÈïøÂ∫¶: {len(response)} Â≠óÁ¨¶")
                        print(f"üì§ ËØ∑Ê±ÇÊàêÂäü")
                        print(f"üì• ÂìçÂ∫îÂÜÖÂÆπ:")
                        print(f"   {response}")
                        print(f"‚úÖ Ê®°Âûã {model_name} ÊµãËØïÊàêÂäü")
                        successful_tests += 1
                    else:
                        logger.warning(f"Ê®°Âûã {model_name} ÊµãËØïÂ§±Ë¥•: ÊâÄÊúâÈáçËØïÈÉΩÂ§±Ë¥•")
                        print(f"‚ùå Ê®°Âûã {model_name} ÊµãËØïÂ§±Ë¥•: ÊâÄÊúâÈáçËØïÈÉΩÂ§±Ë¥•ÔºåËøîÂõû None")
            
            # ÂÖ≥Èó≠ÁÆ°ÁêÜÂô®
            manager.close()
            
        except Exception as e:
            logger.error(f"Êèê‰æõÂïÜ {provider_name} ÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}")
            print(f"‚ùå Êèê‰æõÂïÜ {provider_name} ÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}")
            # Â¶ÇÊûúÊèê‰æõÂïÜÂàùÂßãÂåñÂ§±Ë¥•ÔºåË∑≥ËøáËØ•Êèê‰æõÂïÜÁöÑÊâÄÊúâÊ®°Âûã
            for _ in models:
                total_tests += 1
    
    # ÊòæÁ§∫ÊµãËØïÊÄªÁªì
    success_rate = (successful_tests/total_tests*100) if total_tests > 0 else 0
    logger.info(f"ÊµãËØïÂÆåÊàê - ÊÄªÊï∞: {total_tests}, ÊàêÂäü: {successful_tests}, Â§±Ë¥•: {total_tests - successful_tests}, ÊàêÂäüÁéá: {success_rate:.1f}%")
    
    print("\n" + "=" * 80)
    print("üèÅ ÊµãËØïÂÆåÊàêÊÄªÁªì")
    print("=" * 80)
    print(f"üìä ÊÄªÊµãËØïÊï∞: {total_tests}")
    print(f"‚úÖ ÊàêÂäüÊµãËØï: {successful_tests}")
    print(f"‚ùå Â§±Ë¥•ÊµãËØï: {total_tests - successful_tests}")
    print(f"üìà ÊàêÂäüÁéá: {success_rate:.1f}%")
    
    if successful_tests == 0:
        print("\nüí° ÊèêÁ§∫:")
        print("1. ËØ∑Á°Æ‰øùÂú® .env Êñá‰ª∂‰∏≠ÈÖçÁΩÆ‰∫ÜÁõ∏Â∫îÁöÑ API ÂØÜÈí•")
        print("2. Ê£ÄÊü•ÁΩëÁªúËøûÊé•ÊòØÂê¶Ê≠£Â∏∏")
        print("3. Á°ÆËÆ§ API ÂØÜÈí•ÊúâË∂≥Â§üÁöÑ‰ΩôÈ¢ùÂíåÊùÉÈôê")
        print("4. Ê£ÄÊü•ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑÊ®°ÂûãÂêçÁß∞ÊòØÂê¶Ê≠£Á°Æ")
    elif successful_tests < total_tests:
        print(f"\n‚ö†Ô∏è  Êúâ {total_tests - successful_tests} ‰∏™ÊµãËØïÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü•Áõ∏ÂÖ≥ÈÖçÁΩÆ")
    else:
        print("\nüéâ ÊâÄÊúâÊµãËØïÈÉΩÊàêÂäüÂÆåÊàêÔºÅ")

if __name__ == "__main__":
    # ÂàõÂª∫Êó•ÂøóËÆ∞ÂΩïÂô®
    logger = Logger("logs", "swe_bench_processor")
    logger.info("ÂºÄÂßã SWE-bench Êï∞ÊçÆÂ§ÑÁêÜÂíå LLM API ÊµãËØïÁ®ãÂ∫è")
    
    try:
        # Âä†ËΩΩÈÖçÁΩÆÊñá‰ª∂
        config = load_config(os.getenv("CONFIG_PATH", "config/config.yaml"))
        logger.info(f"ÊàêÂäüÂä†ËΩΩÈÖçÁΩÆÊñá‰ª∂")
        
        # Ê£ÄÊü•ËøêË°åÊ®°Âºè
        run_mode = os.getenv("RUN_MODE", "embedding").lower()  # ÈªòËÆ§ËøêË°åÊï∞ÊçÆÂ§ÑÁêÜÊ®°Âºè
        
        if run_mode == "data":
            logger.info("ËøêË°åÊ®°Âºè: SWE-bench Êï∞ÊçÆÂ§ÑÁêÜ")
            # Â§ÑÁêÜ SWE-bench Êï∞ÊçÆÈõÜ
            swe_loader, result = load_swe_bench_data(config, logger)
        elif run_mode == "llm":
            logger.info("ËøêË°åÊ®°Âºè: LLM API ÊµãËØï")
            # ËøêË°å LLM ÊµãËØï
            test_provider_models(config, logger)
        elif run_mode == "embedding":
            logger.info("ËøêË°åÊ®°Âºè: Embedding ÂäüËÉΩÊµãËØï")
            # ËøêË°å Embedding ÊµãËØï
            test_embedding_functionality(config, logger)
        elif run_mode == "image_builder":
            logger.info("ËøêË°åÊ®°Âºè: SWE-bench Image Builder ÊµãËØï")
            # ËøêË°å Image Builder ÊµãËØï
            # ‰ªéÁéØÂ¢ÉÂèòÈáèËé∑ÂèñÊµãËØïÂèÇÊï∞
            test_instance_ids = os.getenv("TEST_INSTANCE_IDS", "django__django-10914,astropy__astropy-12907").split(",")
            dataset_name = os.getenv("TEST_DATASET_NAME", "SWE-bench/SWE-bench_Lite")
            split = os.getenv("TEST_SPLIT", "test")
            max_workers = calculate_optimal_max_workers()
            
            force_rebuild = os.getenv("TEST_FORCE_REBUILD", "false").lower() == "true"
            timeout = int(os.getenv("TEST_TIMEOUT", "600"))
            
            results = test_swe_image_builder(
                instance_ids=test_instance_ids,
                dataset_name=dataset_name,
                split=split,
                max_workers=max_workers,
                force_rebuild=force_rebuild,
                timeout=timeout,
            )
            
            # ÊòæÁ§∫ÁªìÊûúÊëòË¶Å
            total_instances = len(test_instance_ids)
            successful_instances = len([r for r in results["instance_results"].values() if r.get("success", False)])
            print(f"\nüìä ÊµãËØïÂÆåÊàê: {successful_instances}/{total_instances} ÊàêÂäü")
        elif run_mode == "both":
            logger.info("ËøêË°åÊ®°Âºè: Êï∞ÊçÆÂ§ÑÁêÜ + LLM ÊµãËØï")
            # ÂÖàÂ§ÑÁêÜÊï∞ÊçÆ
            swe_loader, result = load_swe_bench_data(config, logger)
            # ÂÜçËøêË°å LLM ÊµãËØï
            test_provider_models(config, logger)
        elif run_mode == "all":
            logger.info("ËøêË°åÊ®°Âºè: ÂÖ®ÈÉ®ÂäüËÉΩÊµãËØï")
            # ËøêË°åÊâÄÊúâÊµãËØï
            swe_loader, result = load_swe_bench_data(config, logger)
            test_provider_models(config, logger)
            test_embedding_functionality(config, logger)
            test_instance_ids = os.getenv("TEST_INSTANCE_IDS", "django__django-10914").split(",")
            max_workers = calculate_optimal_max_workers()
            results = test_swe_image_builder(
                instance_ids=test_instance_ids,
                dataset_name=os.getenv("TEST_DATASET_NAME", "SWE-bench/SWE-bench_Lite"),
                split=os.getenv("TEST_SPLIT", "test"),
                max_workers=max_workers,
                force_rebuild=os.getenv("TEST_FORCE_REBUILD", "false").lower() == "true",
                timeout=int(os.getenv("TEST_TIMEOUT", "600")),
            )
        else:
            logger.warning(f"Êú™Áü•ËøêË°åÊ®°Âºè: {run_mode}ÔºåÈªòËÆ§ËøêË°åÊï∞ÊçÆÂ§ÑÁêÜ")
            swe_loader, result = load_swe_bench_data(config, logger)
        
    except FileNotFoundError as e:
        logger.error(f"ÈÖçÁΩÆÊñá‰ª∂ÈîôËØØ: {e}")
        print(f"‚ùå ÈÖçÁΩÆÊñá‰ª∂ÈîôËØØ: {e}")
        print("ËØ∑Á°Æ‰øùÂú® src/config/ ÁõÆÂΩï‰∏ãÊúâ config.yaml Êñá‰ª∂")
    except yaml.YAMLError as e:
        logger.error(f"YAML Ëß£ÊûêÈîôËØØ: {e}")
        print(f"‚ùå YAML Ëß£ÊûêÈîôËØØ: {e}")
        print("ËØ∑Ê£ÄÊü• config.yaml Êñá‰ª∂Ê†ºÂºèÊòØÂê¶Ê≠£Á°Æ")
    except Exception as e:
        logger.error(f"Á®ãÂ∫èÊâßË°åÈîôËØØ: {e}")
        print(f"‚ùå Á®ãÂ∫èÊâßË°åÈîôËØØ: {e}")
    finally:
        logger.info("SWE-bench Â§ÑÁêÜÁ®ãÂ∫èÁªìÊùü")
        logger.close()