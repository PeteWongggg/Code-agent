import yaml
import os
from src.managers.llm_api.api_manager import LLMAPIManager
from src.managers.log.logger import Logger
from src.managers.data_loader.load_data import SWEBenchLoader

def load_config(config_path: str = "src/config/config.yaml") -> dict:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

def load_swe_bench_data(config: dict, logger):
    """åŠ è½½å’Œå¤„ç† SWE-bench æ•°æ®é›†"""
    dataset_config = config.get("dataset", {})
    workspace_config = config.get("workspace", {})
    
    dataset_name = dataset_config.get("name", "princeton-nlp/SWE-bench_Lite")
    split_name = dataset_config.get("split", "dev")
    workspace_path = workspace_config.get("path", "workspace")
    
    logger.info(f"å¼€å§‹å¤„ç† SWE-bench æ•°æ®é›†: {dataset_name}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    swe_loader = SWEBenchLoader(
        dataset_name=dataset_name,
        split_name=split_name,
        workspace_path=workspace_path,
        logger=logger
    )
    
    # åŠ è½½å¹¶å¤„ç†æ•°æ®é›†ï¼ˆé™åˆ¶å¤„ç†æ•°é‡ç”¨äºæµ‹è¯•ï¼‰
    max_items = int(os.getenv("MAX_ITEMS", "5"))  # é»˜è®¤å¤„ç†5ä¸ªï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´
    result = swe_loader.load_and_process_all(max_items=max_items)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = swe_loader.get_stats()
    logger.info(f"æ•°æ®å¤„ç†ç»Ÿè®¡: {stats}")
    
    print(f"\nğŸ“Š SWE-bench æ•°æ®å¤„ç†å®Œæˆ:")
    print(f"   æ•°æ®é›†: {dataset_name}")
    print(f"   åˆ†å‰²: {split_name}")
    print(f"   å·¥ä½œç©ºé—´: {workspace_path}")
    print(f"   å¤„ç†ç»“æœ: {result}")
    print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    return swe_loader, result

def test_embedding_functionality(config: dict, logger):
    """æµ‹è¯• embedding åŠŸèƒ½"""
    rag_config = config.get("rag", {})
    embedding_config = rag_config.get("embedding", {})
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ embedding
    if not embedding_config.get("enabled", False):
        logger.info("Embedding åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        print("â­ï¸  Embedding åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    client_name = embedding_config.get("client", "openai")
    model_name = embedding_config.get("model", "text-embedding-3-small")
    
    logger.info(f"å¼€å§‹æµ‹è¯• Embedding åŠŸèƒ½ - å®¢æˆ·ç«¯: {client_name}, æ¨¡å‹: {model_name}")
    
    print("ğŸ§  RAG Embedding åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    print(f"ğŸ”§ é…ç½®: å®¢æˆ·ç«¯={client_name}, æ¨¡å‹={model_name}")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚",
        "Machine learning algorithms can automatically learn and improve from experience.",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å’Œåˆ†æå¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚"
    ]
    
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬ ({len(test_texts)} æ¡):")
    for i, text in enumerate(test_texts, 1):
        print(f"   {i}. {text}")
        logger.debug(f"æµ‹è¯•æ–‡æœ¬ {i}: {text}")
    
    try:
        # ä½¿ç”¨ LLMAPIManager åˆ›å»ºç»Ÿä¸€çš„å®¢æˆ·ç«¯ç®¡ç†å™¨
        manager = LLMAPIManager(
            client_name=client_name,
            timeout=30,
            max_retries=2,
            logger=logger
        )
        
        logger.info(f"æˆåŠŸåˆ›å»º {client_name.upper()} å®¢æˆ·ç«¯ç®¡ç†å™¨")
        print(f"âœ… æˆåŠŸåˆ›å»º {client_name.upper()} å®¢æˆ·ç«¯ç®¡ç†å™¨")
        
        print(f"\nğŸš€ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
        logger.info(f"å¼€å§‹è°ƒç”¨ embedding API - æ¨¡å‹: {model_name}")
        
        # é€šè¿‡ LLMAPIManager è°ƒç”¨ embedding APIï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰
        response = manager.create_embeddings(
            input_text=test_texts,  # ç›´æ¥ä¼ é€’æ–‡æœ¬åˆ—è¡¨ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†
            model=model_name,
            timeout=30,
            retry=2
        )
        
        # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸ
        if response is None:
            error_msg = "Embedding ç”Ÿæˆå¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return
        
        # è®°å½•æˆåŠŸç»“æœ
        logger.info(f"Embedding ç”ŸæˆæˆåŠŸ - æ¨¡å‹: {response.model}, å‘é‡æ•°é‡: {len(response.data)}, Tokenä½¿ç”¨: {response.usage.total_tokens}")
        
        print("âœ… åµŒå…¥å‘é‡ç”ŸæˆæˆåŠŸ!")
        print(f"\nğŸ“Š å“åº”ç»Ÿè®¡:")
        print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {response.model}")
        print(f"   ğŸ“ˆ åµŒå…¥å‘é‡æ•°é‡: {len(response.data)}")
        print(f"   ğŸ”¢ Token ä½¿ç”¨: {response.usage.prompt_tokens} prompt + {response.usage.total_tokens} total")
        
        # æ˜¾ç¤ºæ¯ä¸ªåµŒå…¥å‘é‡çš„è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ” åµŒå…¥å‘é‡è¯¦æƒ…:")
        total_dimensions = 0
        for i, embedding_data in enumerate(response.data):
            vector_dim = len(embedding_data.embedding)
            total_dimensions += vector_dim
            first_few = embedding_data.embedding[:3]  # æ˜¾ç¤ºå‰3ä¸ªå€¼
            last_few = embedding_data.embedding[-3:]  # æ˜¾ç¤ºå3ä¸ªå€¼
            
            print(f"   å‘é‡ {i+1}: ç»´åº¦={vector_dim}")
            print(f"           å‰3ä¸ªå€¼: {[round(x, 6) for x in first_few]}")
            print(f"           å3ä¸ªå€¼: {[round(x, 6) for x in last_few]}")
            
            # è®°å½•åˆ°æ—¥å¿—
            logger.debug(f"å‘é‡ {i+1}: ç»´åº¦={vector_dim}, ç´¢å¼•={embedding_data.index}")
        
        avg_dimension = total_dimensions // len(response.data) if response.data else 0
        logger.info(f"å¹³å‡å‘é‡ç»´åº¦: {avg_dimension}")
        
        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“:")
        print(f"   âœ… æˆåŠŸç”Ÿæˆ {len(response.data)} ä¸ªåµŒå…¥å‘é‡")
        print(f"   ğŸ“ å¹³å‡å‘é‡ç»´åº¦: {avg_dimension}")
        print(f"   âš¡ Token æ•ˆç‡: {response.usage.total_tokens / len(test_texts):.1f} tokens/text")
        print(f"   ğŸ‰ Embedding åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
        
        logger.info("Embedding åŠŸèƒ½æµ‹è¯•æˆåŠŸå®Œæˆ")
        
        # å…³é—­ç®¡ç†å™¨
        manager.close()
        
    except Exception as e:
        error_msg = f"Embedding æµ‹è¯•å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        print(f"âŒ {error_msg}")
        
        # æä¾›æ•…éšœæ’é™¤å»ºè®®
        print(f"\nğŸ’¡ æ•…éšœæ’é™¤å»ºè®®:")
        print(f"   1. æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®")
        print(f"   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
        print(f"   3. éªŒè¯æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®: {model_name}")
        print(f"   4. æ£€æŸ¥ API é…é¢æ˜¯å¦å……è¶³")

def test_provider_models(config: dict, logger):
    """æµ‹è¯•æ‰€æœ‰é…ç½®çš„æä¾›å•†å’Œæ¨¡å‹"""
    providers = config.get("providers", {})
    
    # ç›´æ¥åœ¨ä»£ç ä¸­å®šä¹‰æµ‹è¯•å‚æ•°
    test_message = "å››å¤§åè‘—æœ‰å“ªäº›ï¼Ÿè¯·ç®€è¦ä»‹ç»æ¯ä¸€éƒ¨ã€‚"
    system_message = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„æ–¹å¼å›ç­”é—®é¢˜ã€‚"
    temperature = 0.1
    max_tokens = 500
    stream = False
    timeout = 30
    
    print("ğŸš€ LLM API å¤šæä¾›å•†æ¨¡å‹æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_message}")
    print(f"ğŸ”§ é…ç½®: stream={stream}, temperature={temperature}, max_tokens={max_tokens}")
    print("=" * 80)
    
    total_tests = 0
    successful_tests = 0
    
    # éå†æ¯ä¸ªæä¾›å•†
    for provider_name, models in providers.items():
        logger.info(f"å¼€å§‹æµ‹è¯•æä¾›å•†: {provider_name}")
        print(f"\nğŸ¢ æµ‹è¯•æä¾›å•†: {provider_name.upper()}")
        print("-" * 60)
        
        try:
            # åˆ›å»ºè¯¥æä¾›å•†çš„ç®¡ç†å™¨
            manager = LLMAPIManager(
                client_name=provider_name,
                stream=stream,
                timeout=timeout,
                logger=logger
            )
            
            logger.info(f"{provider_name} å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            print(f"âœ… {provider_name} å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            
            if provider_name == "private":
                model_name = os.getenv("PRIVATE_MODEL_NAME", "qwen-2.5-coder-3b-instruct")
                total_tests += 1
                response = manager.chat(
                    model=model_name,
                    message=test_message,
                    system_message=system_message,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                if response is not None:
                    logger.info(f"æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                    print(f"ğŸ“¤ è¯·æ±‚æˆåŠŸ")
                    print(f"ğŸ“¥ å“åº”å†…å®¹:")
                    print(f"   {response}")
                    print(f"âœ… æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸ")
                    successful_tests += 1
                else:
                    logger.warning(f"æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                    print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
            else:
                # éå†è¯¥æä¾›å•†çš„æ‰€æœ‰æ¨¡å‹
                for model_name in models:
                    total_tests += 1
                    print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹: {model_name}")
                    print("." * 40)
                    
                    # è°ƒç”¨èŠå¤©æ¥å£
                    response = manager.chat(
                        model=model_name,
                        message=test_message,
                        system_message=system_message,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    if response is not None:
                        logger.info(f"æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                        print(f"ğŸ“¤ è¯·æ±‚æˆåŠŸ")
                        print(f"ğŸ“¥ å“åº”å†…å®¹:")
                        print(f"   {response}")
                        print(f"âœ… æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸ")
                        successful_tests += 1
                    else:
                        logger.warning(f"æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                        print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
            
            # å…³é—­ç®¡ç†å™¨
            manager.close()
            
        except Exception as e:
            logger.error(f"æä¾›å•† {provider_name} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print(f"âŒ æä¾›å•† {provider_name} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # å¦‚æœæä¾›å•†åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡è¯¥æä¾›å•†çš„æ‰€æœ‰æ¨¡å‹
            for _ in models:
                total_tests += 1
    
    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    success_rate = (successful_tests/total_tests*100) if total_tests > 0 else 0
    logger.info(f"æµ‹è¯•å®Œæˆ - æ€»æ•°: {total_tests}, æˆåŠŸ: {successful_tests}, å¤±è´¥: {total_tests - successful_tests}, æˆåŠŸç‡: {success_rate:.1f}%")
    
    print("\n" + "=" * 80)
    print("ğŸ æµ‹è¯•å®Œæˆæ€»ç»“")
    print("=" * 80)
    print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
    
    if successful_tests == 0:
        print("\nğŸ’¡ æç¤º:")
        print("1. è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­é…ç½®äº†ç›¸åº”çš„ API å¯†é’¥")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. ç¡®è®¤ API å¯†é’¥æœ‰è¶³å¤Ÿçš„ä½™é¢å’Œæƒé™")
        print("4. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
    elif successful_tests < total_tests:
        print(f"\nâš ï¸  æœ‰ {total_tests - successful_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®")
    else:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸå®Œæˆï¼")

if __name__ == "__main__":
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = Logger("logs", "swe_bench_processor")
    logger.info("å¼€å§‹ SWE-bench æ•°æ®å¤„ç†å’Œ LLM API æµ‹è¯•ç¨‹åº")
    
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        config = load_config(os.getenv("CONFIG_PATH", "config/config.yaml"))
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶")
        
        # æ£€æŸ¥è¿è¡Œæ¨¡å¼
        run_mode = os.getenv("RUN_MODE", "embedding").lower()  # é»˜è®¤è¿è¡Œæ•°æ®å¤„ç†æ¨¡å¼
        
        if run_mode == "data":
            logger.info("è¿è¡Œæ¨¡å¼: SWE-bench æ•°æ®å¤„ç†")
            # å¤„ç† SWE-bench æ•°æ®é›†
            swe_loader, result = load_swe_bench_data(config, logger)
        elif run_mode == "llm":
            logger.info("è¿è¡Œæ¨¡å¼: LLM API æµ‹è¯•")
            # è¿è¡Œ LLM æµ‹è¯•
            test_provider_models(config, logger)
        elif run_mode == "embedding":
            logger.info("è¿è¡Œæ¨¡å¼: Embedding åŠŸèƒ½æµ‹è¯•")
            # è¿è¡Œ Embedding æµ‹è¯•
            test_embedding_functionality(config, logger)
        elif run_mode == "both":
            logger.info("è¿è¡Œæ¨¡å¼: æ•°æ®å¤„ç† + LLM æµ‹è¯•")
            # å…ˆå¤„ç†æ•°æ®
            swe_loader, result = load_swe_bench_data(config, logger)
            # å†è¿è¡Œ LLM æµ‹è¯•
            test_provider_models(config, logger)
        elif run_mode == "all":
            logger.info("è¿è¡Œæ¨¡å¼: å…¨éƒ¨åŠŸèƒ½æµ‹è¯•")
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            swe_loader, result = load_swe_bench_data(config, logger)
            test_provider_models(config, logger)
            test_embedding_functionality(config, logger)
        else:
            logger.warning(f"æœªçŸ¥è¿è¡Œæ¨¡å¼: {run_mode}ï¼Œé»˜è®¤è¿è¡Œæ•°æ®å¤„ç†")
            swe_loader, result = load_swe_bench_data(config, logger)
        
    except FileNotFoundError as e:
        logger.error(f"é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿åœ¨ src/config/ ç›®å½•ä¸‹æœ‰ config.yaml æ–‡ä»¶")
    except yaml.YAMLError as e:
        logger.error(f"YAML è§£æé”™è¯¯: {e}")
        print(f"âŒ YAML è§£æé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ config.yaml æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
    finally:
        logger.info("SWE-bench å¤„ç†ç¨‹åºç»“æŸ")
        logger.close()