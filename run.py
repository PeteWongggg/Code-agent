import yaml
import os
import time
import traceback
import platform
import psutil
from pathlib import Path
from typing import Dict, Any, List, Optional
import docker

from src.managers.llm_api.api_manager import LLMAPIManager
from src.managers.log.logger import Logger
from src.managers.image_builder.build_image import SWEBenchLoader

# Import for SWE-bench image builder testing
from src.managers.image_builder.build_image import SWEBenchImageBuilder
from swebench.harness.utils import load_swebench_dataset, EvaluationError
from swebench.harness.test_spec.test_spec import make_test_spec
from swebench.harness.docker_build import (
    build_env_images, 
    build_instance_image, 
    setup_logger, 
    close_logger
)
from swebench.harness.docker_utils import (
    cleanup_container,
    copy_to_container,
    exec_run_with_timeout,
)
from swebench.harness.grading import get_eval_report, get_logs_eval
from swebench.harness.constants import (
    APPLY_PATCH_FAIL,
    APPLY_PATCH_PASS,
    DOCKER_PATCH,
    DOCKER_USER,
    DOCKER_WORKDIR,
    KEY_INSTANCE_ID,
    KEY_PREDICTION,
    LOG_TEST_OUTPUT,
    UTF8,
)

# Git apply commands for patch application
GIT_APPLY_CMDS = [
    "git apply --verbose",
    "git apply --verbose --reject",
    "patch --batch --fuzz=5 -p1 -i",
]


def calculate_optimal_max_workers(ram_gb_per_worker: int = 6) -> int:
    """
    Calculate optimal max_workers based on system RAM.
    
    Args:
        ram_gb_per_worker: GB of RAM required per worker (default: 6GB)
        
    Returns:
        int: Optimal number of workers, minimum 1, maximum 8
    """
    try:
        # Get system memory in bytes
        memory = psutil.virtual_memory()
        total_ram_gb = memory.total / (1024**3)  # Convert bytes to GB
        
        # Calculate max workers based on RAM
        calculated_workers = int(total_ram_gb / ram_gb_per_worker)
        
        # Apply reasonable bounds: minimum 1, maximum 8
        optimal_workers = max(1, min(calculated_workers, 8))
        
        return optimal_workers
        
    except Exception as e:
        print(f"âš ï¸  Could not detect system RAM: {e}")
        print("ğŸ”§ Using default max_workers: 1")
        return 1

def format_test_results(tests_status: Dict[str, Any]) -> Dict[str, Any]:
    """
    Helper function to format test results for display.
    
    Args:
        tests_status: The tests_status dictionary from evaluation report
        
    Returns:
        Dictionary with formatted test results
    """
    f2p_success = len(tests_status.get("FAIL_TO_PASS", {}).get("success", []))
    f2p_failure = len(tests_status.get("FAIL_TO_PASS", {}).get("failure", []))
    p2p_success = len(tests_status.get("PASS_TO_PASS", {}).get("success", []))
    p2p_failure = len(tests_status.get("PASS_TO_PASS", {}).get("failure", []))
    
    return {
        "fail_to_pass": {
            "success": f2p_success,
            "failure": f2p_failure,
            "total": f2p_success + f2p_failure,
            "passed": f2p_failure == 0 and f2p_success > 0
        },
        "pass_to_pass": {
            "success": p2p_success,
            "failure": p2p_failure,
            "total": p2p_success + p2p_failure,
            "passed": p2p_failure == 0 and p2p_success > 0
        }
    }


def run_tests_on_container(
    container,
    test_spec,
    log_dir: Path,
    logger,
    timeout: int,
    test_prefix: str = ""
) -> tuple[Dict[str, Any], Optional[Dict[str, Any]], str]:
    """
    Helper function to run tests on a container and return results.
    
    Returns:
        tuple: (test_results, evaluation_report, test_output)
    """
    # Write evaluation script to container
    eval_file = log_dir / f"{test_prefix}eval.sh"
    eval_file.write_text(test_spec.eval_script, encoding=UTF8)
    copy_to_container(container, eval_file, Path("/root/eval.sh"))
    
    # Prepare the run command
    run_command = "/bin/bash /root/eval.sh"
    
    # Execute the evaluation with timeout
    test_output, timed_out, exec_time = exec_run_with_timeout(
        container, run_command, timeout
    )
    
    # Save test output to file
    test_output_file = log_dir / f"{test_prefix}{LOG_TEST_OUTPUT}"
    test_output_file.write_text(test_output, encoding=UTF8)
    
    # Parse test results
    eval_status_map, found = get_logs_eval(test_spec, str(test_output_file))
    
    test_results = None
    if not found:
        test_results = {"status": "parse_failed", "output": test_output}
    else:
        test_results = eval_status_map
    
    # Generate evaluation report
    prediction = {
        KEY_INSTANCE_ID: test_spec.instance_id,
        KEY_PREDICTION: "",  # Empty prediction for pre-patch test
    }
    
    evaluation_report = get_eval_report(
        test_spec,
        prediction,
        str(test_output_file),
        include_tests_status=True,
    )
    
    return test_results, evaluation_report, test_output


def test_image(
    image_name: str,
    instance_id: str,
    dataset_name: str = "SWE-bench/SWE-bench_Lite",
    split: str = "test",
    timeout: int = 600,
    log_dir: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Tests a patch on an existing Docker image by running tests both before and after applying the gold patch.
    """
    start_time = time.time()
    result = {
        "instance_id": instance_id,
        "image_name": image_name,
        "patch_applied": False,
        "pre_patch_tests_passed": False,
        "post_patch_tests_passed": False,
        "pre_patch_test_results": None,
        "post_patch_test_results": None,
        "pre_patch_evaluation_report": None,
        "post_patch_evaluation_report": None,
        "pre_patch_terminal_output": None,
        "post_patch_terminal_output": None,
        "error": None,
        "logs": None,
    }
    
    try:
        # Load the specific instance from the dataset to get the gold patch
        dataset = load_swebench_dataset(dataset_name, split, [instance_id])
        
        if not dataset:
            raise ValueError(f"Instance {instance_id} not found in dataset {dataset_name}")
        
        instance = dataset[0]
        
        # Create test spec from the instance
        test_spec = make_test_spec(instance)
        
        # Set up Docker client
        client = docker.from_env()
        
        # Set up log directory
        if log_dir is None:
            log_dir = Path.cwd() / "logs" / "test_image" / instance_id
        else:
            log_dir = Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        result["logs"] = str(log_dir)
        
        # Set up logger
        log_file = log_dir / "test_image.log"
        logger = setup_logger(instance_id, log_file)
        
        try:
            # Check if image exists
            try:
                client.images.get(image_name)
            except docker.errors.ImageNotFound:
                raise ValueError(f"Image {image_name} not found")
            
            # Create container from the image
            container_name = f"test_{instance_id}_{int(time.time())}"
            
            # Get run args and platform from test_spec
            run_args = test_spec.docker_specs.get("run_args", {})
            cap_add = run_args.get("cap_add", [])
            
            container = client.containers.create(
                image_name,
                name=container_name,
                working_dir=DOCKER_WORKDIR,
                user=DOCKER_USER,
                detach=True,
                command="tail -f /dev/null",  # Keep container running
                platform=test_spec.platform,  # Ensure correct architecture
                cap_add=cap_add,  # Add capabilities if needed
            )
            
            try:
                # Start the container
                container.start()
                
                # Run tests BEFORE applying the patch
                pre_patch_test_results, pre_patch_evaluation_report, pre_patch_output = run_tests_on_container(
                    container, test_spec, log_dir, logger, timeout, "pre_patch_"
                )
                
                result["pre_patch_test_results"] = pre_patch_test_results
                result["pre_patch_evaluation_report"] = pre_patch_evaluation_report
                result["pre_patch_terminal_output"] = pre_patch_output
                
                # Check if pre-patch tests passed and show results
                if pre_patch_evaluation_report:
                    instance_report = pre_patch_evaluation_report.get(instance_id, {})
                    result["pre_patch_tests_passed"] = instance_report.get("resolved", False)
                    
                    # Show detailed results
                    if "tests_status" in instance_report:
                        tests_status = instance_report["tests_status"]
                        formatted_results = format_test_results(tests_status)
                        result["pre_patch_test_details"] = formatted_results
                        
                        f2p = formatted_results["fail_to_pass"]
                        p2p = formatted_results["pass_to_pass"]
                        
                        print(f"Pre-patch test results:")
                        print(f"  FAIL_TO_PASS: {f2p['success']}/{f2p['total']} passed {'âœ…' if f2p['passed'] else 'âŒ'}")
                        print(f"  PASS_TO_PASS: {p2p['success']}/{p2p['total']} passed {'âœ…' if p2p['passed'] else 'âŒ'}")
                    else:
                        result["pre_patch_test_details"] = None
                        print("Pre-patch: Could not determine detailed test results")
                else:
                    result["pre_patch_test_details"] = None
                    print("Pre-patch: Could not determine test results")
                
                # Get the gold patch from the instance
                gold_patch = instance.get("patch", "")
                if not gold_patch:
                    raise ValueError(f"No gold patch found for instance {instance_id}")
                
                # Write patch to file and copy to container
                patch_file = log_dir / "gold_patch.diff"
                patch_file.write_text(gold_patch, encoding=UTF8)
                logger.info(f"Gold patch written to {patch_file}")
                
                copy_to_container(container, patch_file, Path(DOCKER_PATCH))
                logger.info(f"Gold patch copied to container at {DOCKER_PATCH}")
                
                # Apply the gold patch to the container
                print("Applying gold patch to container...")
                applied_patch = False
                for git_apply_cmd in GIT_APPLY_CMDS:
                    try:
                        exit_code, output = container.exec_run(
                            f"{git_apply_cmd} {DOCKER_PATCH}",
                            workdir=DOCKER_WORKDIR,
                            user=DOCKER_USER,
                        )
                        if exit_code == 0:
                            logger.info(f"{APPLY_PATCH_PASS}:\n{output.decode(UTF8)}")
                            applied_patch = True
                            break
                        else:
                            logger.info(f"Failed to apply patch with {git_apply_cmd}: {output.decode(UTF8)}")
                    except Exception as e:
                        logger.info(f"Error applying patch with {git_apply_cmd}: {str(e)}")
                        continue
                
                if not applied_patch:
                    error_msg = f"{APPLY_PATCH_FAIL}: Could not apply gold patch with any method"
                    logger.error(error_msg)
                    raise EvaluationError(instance_id, error_msg, logger)
                
                result["patch_applied"] = True
                print("âœ… Gold patch applied successfully")
                
                # Get git diff to see what changed
                _, git_diff_bytes = container.exec_run(
                    "git -c core.fileMode=false diff", workdir=DOCKER_WORKDIR
                )
                git_diff_output = git_diff_bytes.decode(UTF8).strip()
                logger.info(f"Git diff after patch application:\n{git_diff_output}")
                
                # Run tests AFTER applying the patch
                print("Running tests after applying patch...")
                post_patch_test_results, post_patch_evaluation_report, post_patch_output = run_tests_on_container(
                    container, test_spec, log_dir, logger, timeout, "post_patch_"
                )
                
                result["post_patch_test_results"] = post_patch_test_results
                result["post_patch_evaluation_report"] = post_patch_evaluation_report
                result["post_patch_terminal_output"] = post_patch_output
                
                # Check if post-patch tests passed
                if post_patch_evaluation_report:
                    instance_report = post_patch_evaluation_report.get(instance_id, {})
                    result["post_patch_tests_passed"] = instance_report.get("resolved", False)
                    
                    # Show detailed results
                    if "tests_status" in instance_report:
                        tests_status = instance_report["tests_status"]
                        formatted_results = format_test_results(tests_status)
                        result["post_patch_test_details"] = formatted_results
                        
                        f2p = formatted_results["fail_to_pass"]
                        p2p = formatted_results["pass_to_pass"]
                        
                        print(f"Post-patch test results:")
                        print(f"  FAIL_TO_PASS: {f2p['success']}/{f2p['total']} passed {'âœ…' if f2p['passed'] else 'âŒ'}")
                        print(f"  PASS_TO_PASS: {p2p['success']}/{p2p['total']} passed {'âœ…' if p2p['passed'] else 'âŒ'}")
                        
                        if result["post_patch_tests_passed"]:
                            print("âœ… All post-patch tests passed!")
                        else:
                            print("âŒ Some post-patch tests failed")
                    else:
                        result["post_patch_test_details"] = None
                        print("Post-patch: Could not determine detailed test results")
                else:
                    result["post_patch_test_details"] = None
                    print("âš ï¸  Could not determine post-patch test results")
                
            finally:
                # Clean up container
                cleanup_container(client, container, logger)
                
        finally:
            close_logger(logger)
            
    except Exception as e:
        error_msg = f"Error testing image {image_name} with instance {instance_id}: {str(e)}"
        print(f"âŒ {error_msg}")
        result["error"] = error_msg
        print(f"Traceback: {traceback.format_exc()}")
    
    return result


def test_swe_image_builder(
    instance_ids: List[str],
    dataset_name: str = "SWE-bench/SWE-bench_Lite",
    split: str = "test",
    max_workers: int = 2,
    force_rebuild: bool = False,
    timeout: int = 600,
) -> Dict[str, Any]:
    """
    Test the SWEBenchImageBuilder by building images for multiple instances and running evaluations.
    
    This function provides a comprehensive testing framework for the SWEBenchImageBuilder class.
    It performs the following operations:
    
    1. **Image Building**: Uses SWEBenchImageBuilder to build Docker images for specified SWE-bench instances
    2. **Pre-patch Testing**: Runs tests on the built images before applying any patches
    3. **Patch Application**: Applies the gold patch from the SWE-bench dataset to the container
    4. **Post-patch Testing**: Runs tests again after patch application to verify the fix
    5. **Result Analysis**: Compares pre-patch and post-patch test results to determine success
    
    Usage Examples:
    
    # Basic usage with default parameters
    results = test_swe_image_builder(["django__django-10914"])
    
    # Test multiple instances with custom settings
    results = test_swe_image_builder(
        instance_ids=["django__django-10914", "astropy__astropy-12907"],
        dataset_name="SWE-bench/SWE-bench",
        split="test",
        max_workers=1,
        force_rebuild=True,
        timeout=900
    )
    
    # Test with SWE-bench Lite dataset
    results = test_swe_image_builder(
        instance_ids=["django__django-10914"],
        dataset_name="SWE-bench/SWE-bench_Lite",
        split="dev"
    )
    
    Args:
        instance_ids (List[str]): List of SWE-bench instance IDs to build and test.
                                 Format: "repository__repository-issue_number"
                                 Example: ["django__django-10914", "astropy__astropy-12907"]
        
        dataset_name (str, optional): Name of the SWE-bench dataset to use.
                                    Defaults to "SWE-bench/SWE-bench_Lite".
                                    Options: "SWE-bench/SWE-bench", "SWE-bench/SWE-bench_Lite"
        
        split (str, optional): Dataset split to use. Defaults to "test".
                              Options: "dev", "test"
        
        max_workers (int, optional): Number of parallel workers for image building.
                                   Defaults to 2. Use 1 for sequential building to avoid memory issues.
        
        force_rebuild (bool, optional): Whether to force rebuild existing images.
                                      Defaults to False. Set to True to rebuild even if images exist.
        
        timeout (int, optional): Timeout in seconds for test execution. Defaults to 600 (10 minutes).
    
    Returns:
        Dict[str, Any]: Comprehensive test results containing:
            - instance_results: Dictionary mapping instance_id to detailed results
            - errors: List of error messages encountered during testing
            
        Each instance result contains:
            - image_name: Name of the built Docker image
            - build_status: Status of the image build process
            - test_result: Detailed test results including:
                - patch_applied: Whether the gold patch was successfully applied
                - pre_patch_tests_passed: Whether tests passed before patch application
                - post_patch_tests_passed: Whether tests passed after patch application
                - pre_patch_test_details: Detailed breakdown of pre-patch test results
                - post_patch_test_details: Detailed breakdown of post-patch test results
                - logs: Path to log files for debugging
    
    Test Result Interpretation:
        - âœ… PERFECT: Pre-patch tests failed, post-patch tests passed (patch fixed the issue)
        - âœ… GOOD: All tests pass both before and after patch (no regression)
        - âš ï¸  WARNING: Pre-patch tests passed, post-patch tests failed (patch broke something)
        - âŒ ISSUE: Tests fail both before and after patch (patch didn't fix the issue)
    
    Requirements:
        - Docker must be installed and running
        - SWE-bench package must be installed
        - Sufficient disk space for Docker images (several GB per instance)
        - Network access to download datasets and base images
    
    Error Handling:
        - Gracefully handles Docker image build failures
        - Continues testing other instances if one fails
        - Provides detailed error messages and stack traces
        - Logs all operations for debugging purposes
    
    Performance Notes:
        - Image building can take 10-30 minutes per instance
        - Test execution typically takes 5-15 minutes per instance
        - Use max_workers=1 for memory-constrained environments
        - Consider using SWE-bench_Lite for faster testing with smaller instances
    """
    start_time = time.time()
    results = {
        "instance_results": {},
        "errors": []
    }
    
    try:
        print(f"ğŸš€ Starting SWEBenchImageBuilder test with {len(instance_ids)} instances")
        print(f"Instance IDs: {instance_ids}")
        
        # Step 1: Build images using SWEBenchImageBuilder
        print("\nğŸ“¦ Step 1: Building images with SWEBenchImageBuilder...")
        builder = SWEBenchImageBuilder(
            dataset_name=dataset_name,
            split=split,
            instance_ids=instance_ids,
            max_workers=max_workers,
            force_rebuild=force_rebuild,
        )
        
        # Step 2: Test each successfully built image
        print("\nğŸ§ª Step 2: Testing built images...")
        
        for instance_id in instance_ids:
            print(f"\n--- Testing instance: {instance_id} ---")
            
            try:
                # Get image name from builder
                image_name = builder.get_image_name(instance_id)
                build_status = builder.get_build_status(instance_id)
                
                print(f"Image name: {image_name}")
                print(f"Build status: {build_status}")
                
                if build_status in ['successful', 'already_exists']:
                    # Test the image
                    test_result = test_image(
                        image_name=image_name,
                        instance_id=instance_id,
                        dataset_name=dataset_name,
                        split=split,
                        timeout=timeout,
                    )
                    
                    results["instance_results"][instance_id] = {
                        "image_name": image_name,
                        "build_status": build_status,
                        "test_result": test_result,
                        "success": test_result.get("patch_applied", False)
                    }
                    
                    if test_result.get("patch_applied", False):
                        print(f"âœ… {instance_id}: Image built and patch applied successfully")
                        
                        # Show test results summary
                        pre_patch_passed = test_result.get('pre_patch_tests_passed', False)
                        post_patch_passed = test_result.get('post_patch_tests_passed', False)
                        pre_patch_details = test_result.get('pre_patch_test_details')
                        post_patch_details = test_result.get('post_patch_test_details')
                        
                        print(f"  Pre-patch tests: {'âœ… PASSED' if pre_patch_passed else 'âŒ FAILED'}")
                        if pre_patch_details:
                            f2p = pre_patch_details["fail_to_pass"]
                            p2p = pre_patch_details["pass_to_pass"]
                            print(f"    FAIL_TO_PASS: {f2p['success']}/{f2p['total']} {'âœ…' if f2p['passed'] else 'âŒ'}")
                            print(f"    PASS_TO_PASS: {p2p['success']}/{p2p['total']} {'âœ…' if p2p['passed'] else 'âŒ'}")
                        
                        print(f"  Post-patch tests: {'âœ… PASSED' if post_patch_passed else 'âŒ FAILED'}")
                        if post_patch_details:
                            f2p = post_patch_details["fail_to_pass"]
                            p2p = post_patch_details["pass_to_pass"]
                            print(f"    FAIL_TO_PASS: {f2p['success']}/{f2p['total']} {'âœ…' if f2p['passed'] else 'âŒ'}")
                            print(f"    PASS_TO_PASS: {p2p['success']}/{p2p['total']} {'âœ…' if p2p['passed'] else 'âŒ'}")
                        
                        if not pre_patch_passed and post_patch_passed:
                            print("  ğŸ¯ PERFECT: Patch fixed failing tests!")
                        elif pre_patch_passed and post_patch_passed:
                            print("  âœ… GOOD: All tests pass both before and after patch")
                        elif pre_patch_passed and not post_patch_passed:
                            print("  âš ï¸  WARNING: Patch broke previously passing tests")
                        else:
                            print("  âŒ ISSUE: Tests fail both before and after patch")
                    else:
                        print(f"âŒ {instance_id}: Patch could not be applied")
                        results["errors"].append(f"{instance_id}: Patch application failed")
                        
                else:
                    print(f"âŒ {instance_id}: Image build failed (status: {build_status})")
                    results["instance_results"][instance_id] = {
                        "image_name": None,
                        "build_status": build_status,
                        "test_result": None,
                        "success": False
                    }
                    results["errors"].append(f"{instance_id}: Image build failed")
                    
            except Exception as e:
                error_msg = f"Error testing instance {instance_id}: {str(e)}"
                print(f"âŒ {error_msg}")
                results["errors"].append(error_msg)
                results["instance_results"][instance_id] = {
                    "image_name": None,
                    "build_status": "error",
                    "test_result": None,
                    "success": False,
                    "error": error_msg
                }
        
    except Exception as e:
        error_msg = f"Error in test_swe_image_builder: {str(e)}"
        print(f"âŒ {error_msg}")
        results["errors"].append(error_msg)
        print(f"Traceback: {traceback.format_exc()}")
    
    return results


def load_config(config_path: str = "src/config/config.yaml") -> dict:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

def load_swe_bench_data(config: dict, logger):
    """åŠ è½½å’Œå¤„ç† SWE-bench æ•°æ®é›†"""
    dataset_config = config.get("dataset", {})
    workspace_config = config.get("workspace", {})
    
    dataset_name = dataset_config.get("name", "princeton-nlp/SWE-bench_Lite")
    split_name = dataset_config.get("split", "dev")
    workspace_path = workspace_config.get("path", "workspace")
    
    logger.info(f"å¼€å§‹å¤„ç† SWE-bench æ•°æ®é›†: {dataset_name}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    swe_loader = SWEBenchLoader(
        dataset_name=dataset_name,
        split_name=split_name,
        workspace_path=workspace_path,
        logger=logger
    )
    
    # åŠ è½½å¹¶å¤„ç†æ•°æ®é›†ï¼ˆé™åˆ¶å¤„ç†æ•°é‡ç”¨äºæµ‹è¯•ï¼‰
    max_items = int(os.getenv("MAX_ITEMS", "5"))  # é»˜è®¤å¤„ç†5ä¸ªï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´
    result = swe_loader.load_and_process_all(max_items=max_items)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = swe_loader.get_stats()
    logger.info(f"æ•°æ®å¤„ç†ç»Ÿè®¡: {stats}")
    
    print(f"\nğŸ“Š SWE-bench æ•°æ®å¤„ç†å®Œæˆ:")
    print(f"   æ•°æ®é›†: {dataset_name}")
    print(f"   åˆ†å‰²: {split_name}")
    print(f"   å·¥ä½œç©ºé—´: {workspace_path}")
    print(f"   å¤„ç†ç»“æœ: {result}")
    print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    return swe_loader, result

def test_embedding_functionality(config: dict, logger):
    """æµ‹è¯• embedding åŠŸèƒ½"""
    rag_config = config.get("rag", {})
    embedding_config = rag_config.get("embedding", {})
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ embedding
    if not embedding_config.get("enabled", False):
        logger.info("Embedding åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        print("â­ï¸  Embedding åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    client_name = embedding_config.get("client", "openai")
    model_name = embedding_config.get("model", "text-embedding-3-small")
    
    logger.info(f"å¼€å§‹æµ‹è¯• Embedding åŠŸèƒ½ - å®¢æˆ·ç«¯: {client_name}, æ¨¡å‹: {model_name}")
    
    print("ğŸ§  RAG Embedding åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    print(f"ğŸ”§ é…ç½®: å®¢æˆ·ç«¯={client_name}, æ¨¡å‹={model_name}")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚",
        "Machine learning algorithms can automatically learn and improve from experience.",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å’Œåˆ†æå¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚"
    ]
    
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬ ({len(test_texts)} æ¡):")
    for i, text in enumerate(test_texts, 1):
        print(f"   {i}. {text}")
        logger.debug(f"æµ‹è¯•æ–‡æœ¬ {i}: {text}")
    
    try:
        # ä½¿ç”¨ LLMAPIManager åˆ›å»ºç»Ÿä¸€çš„å®¢æˆ·ç«¯ç®¡ç†å™¨
        manager = LLMAPIManager(
            client_name=client_name,
            timeout=30,
            max_retries=2,
            logger=logger
        )
        
        logger.info(f"æˆåŠŸåˆ›å»º {client_name.upper()} å®¢æˆ·ç«¯ç®¡ç†å™¨")
        print(f"âœ… æˆåŠŸåˆ›å»º {client_name.upper()} å®¢æˆ·ç«¯ç®¡ç†å™¨")
        
        print(f"\nğŸš€ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
        logger.info(f"å¼€å§‹è°ƒç”¨ embedding API - æ¨¡å‹: {model_name}")
        
        # é€šè¿‡ LLMAPIManager è°ƒç”¨ embedding APIï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰
        response = manager.create_embeddings(
            input_text=test_texts,  # ç›´æ¥ä¼ é€’æ–‡æœ¬åˆ—è¡¨ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†
            model=model_name,
            timeout=30,
            retry=2
        )
        
        # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸ
        if response is None:
            error_msg = "Embedding ç”Ÿæˆå¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return
        
        # è®°å½•æˆåŠŸç»“æœ
        logger.info(f"Embedding ç”ŸæˆæˆåŠŸ - æ¨¡å‹: {response.model}, å‘é‡æ•°é‡: {len(response.data)}, Tokenä½¿ç”¨: {response.usage.total_tokens}")
        
        print("âœ… åµŒå…¥å‘é‡ç”ŸæˆæˆåŠŸ!")
        print(f"\nğŸ“Š å“åº”ç»Ÿè®¡:")
        print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {response.model}")
        print(f"   ğŸ“ˆ åµŒå…¥å‘é‡æ•°é‡: {len(response.data)}")
        print(f"   ğŸ”¢ Token ä½¿ç”¨: {response.usage.prompt_tokens} prompt + {response.usage.total_tokens} total")
        
        # æ˜¾ç¤ºæ¯ä¸ªåµŒå…¥å‘é‡çš„è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ” åµŒå…¥å‘é‡è¯¦æƒ…:")
        total_dimensions = 0
        for i, embedding_data in enumerate(response.data):
            vector_dim = len(embedding_data.embedding)
            total_dimensions += vector_dim
            first_few = embedding_data.embedding[:3]  # æ˜¾ç¤ºå‰3ä¸ªå€¼
            last_few = embedding_data.embedding[-3:]  # æ˜¾ç¤ºå3ä¸ªå€¼
            
            print(f"   å‘é‡ {i+1}: ç»´åº¦={vector_dim}")
            print(f"           å‰3ä¸ªå€¼: {[round(x, 6) for x in first_few]}")
            print(f"           å3ä¸ªå€¼: {[round(x, 6) for x in last_few]}")
            
            # è®°å½•åˆ°æ—¥å¿—
            logger.debug(f"å‘é‡ {i+1}: ç»´åº¦={vector_dim}, ç´¢å¼•={embedding_data.index}")
        
        avg_dimension = total_dimensions // len(response.data) if response.data else 0
        logger.info(f"å¹³å‡å‘é‡ç»´åº¦: {avg_dimension}")
        
        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“:")
        print(f"   âœ… æˆåŠŸç”Ÿæˆ {len(response.data)} ä¸ªåµŒå…¥å‘é‡")
        print(f"   ğŸ“ å¹³å‡å‘é‡ç»´åº¦: {avg_dimension}")
        print(f"   âš¡ Token æ•ˆç‡: {response.usage.total_tokens / len(test_texts):.1f} tokens/text")
        print(f"   ğŸ‰ Embedding åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
        
        logger.info("Embedding åŠŸèƒ½æµ‹è¯•æˆåŠŸå®Œæˆ")
        
        # å…³é—­ç®¡ç†å™¨
        manager.close()
        
    except Exception as e:
        error_msg = f"Embedding æµ‹è¯•å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        print(f"âŒ {error_msg}")
        
        # æä¾›æ•…éšœæ’é™¤å»ºè®®
        print(f"\nğŸ’¡ æ•…éšœæ’é™¤å»ºè®®:")
        print(f"   1. æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®")
        print(f"   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
        print(f"   3. éªŒè¯æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®: {model_name}")
        print(f"   4. æ£€æŸ¥ API é…é¢æ˜¯å¦å……è¶³")

def test_provider_models(config: dict, logger):
    """æµ‹è¯•æ‰€æœ‰é…ç½®çš„æä¾›å•†å’Œæ¨¡å‹"""
    providers = config.get("providers", {})
    
    # ç›´æ¥åœ¨ä»£ç ä¸­å®šä¹‰æµ‹è¯•å‚æ•°
    test_message = "å››å¤§åè‘—æœ‰å“ªäº›ï¼Ÿè¯·ç®€è¦ä»‹ç»æ¯ä¸€éƒ¨ã€‚"
    system_message = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„æ–¹å¼å›ç­”é—®é¢˜ã€‚"
    temperature = 0.1
    max_tokens = 500
    stream = False
    timeout = 30
    
    print("ğŸš€ LLM API å¤šæä¾›å•†æ¨¡å‹æµ‹è¯•")
    print("=" * 80)
    print(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_message}")
    print(f"ğŸ”§ é…ç½®: stream={stream}, temperature={temperature}, max_tokens={max_tokens}")
    print("=" * 80)
    
    total_tests = 0
    successful_tests = 0
    
    # éå†æ¯ä¸ªæä¾›å•†
    for provider_name, models in providers.items():
        logger.info(f"å¼€å§‹æµ‹è¯•æä¾›å•†: {provider_name}")
        print(f"\nğŸ¢ æµ‹è¯•æä¾›å•†: {provider_name.upper()}")
        print("-" * 60)
        
        try:
            # åˆ›å»ºè¯¥æä¾›å•†çš„ç®¡ç†å™¨
            manager = LLMAPIManager(
                client_name=provider_name,
                stream=stream,
                timeout=timeout,
                logger=logger
            )
            
            logger.info(f"{provider_name} å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            print(f"âœ… {provider_name} å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            
            if provider_name == "private":
                model_name = os.getenv("PRIVATE_MODEL_NAME", "qwen-2.5-coder-3b-instruct")
                total_tests += 1
                response = manager.chat(
                    model=model_name,
                    message=test_message,
                    system_message=system_message,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                if response is not None:
                    logger.info(f"æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                    print(f"ğŸ“¤ è¯·æ±‚æˆåŠŸ")
                    print(f"ğŸ“¥ å“åº”å†…å®¹:")
                    print(f"   {response}")
                    print(f"âœ… æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸ")
                    successful_tests += 1
                else:
                    logger.warning(f"æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                    print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
            else:
                # éå†è¯¥æä¾›å•†çš„æ‰€æœ‰æ¨¡å‹
                for model_name in models:
                    total_tests += 1
                    print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹: {model_name}")
                    print("." * 40)
                    
                    # è°ƒç”¨èŠå¤©æ¥å£
                    response = manager.chat(
                        model=model_name,
                        message=test_message,
                        system_message=system_message,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    if response is not None:
                        logger.info(f"æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                        print(f"ğŸ“¤ è¯·æ±‚æˆåŠŸ")
                        print(f"ğŸ“¥ å“åº”å†…å®¹:")
                        print(f"   {response}")
                        print(f"âœ… æ¨¡å‹ {model_name} æµ‹è¯•æˆåŠŸ")
                        successful_tests += 1
                    else:
                        logger.warning(f"æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
                        print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› None")
            
            # å…³é—­ç®¡ç†å™¨
            manager.close()
            
        except Exception as e:
            logger.error(f"æä¾›å•† {provider_name} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            print(f"âŒ æä¾›å•† {provider_name} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            # å¦‚æœæä¾›å•†åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡è¯¥æä¾›å•†çš„æ‰€æœ‰æ¨¡å‹
            for _ in models:
                total_tests += 1
    
    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    success_rate = (successful_tests/total_tests*100) if total_tests > 0 else 0
    logger.info(f"æµ‹è¯•å®Œæˆ - æ€»æ•°: {total_tests}, æˆåŠŸ: {successful_tests}, å¤±è´¥: {total_tests - successful_tests}, æˆåŠŸç‡: {success_rate:.1f}%")
    
    print("\n" + "=" * 80)
    print("ğŸ æµ‹è¯•å®Œæˆæ€»ç»“")
    print("=" * 80)
    print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
    
    if successful_tests == 0:
        print("\nğŸ’¡ æç¤º:")
        print("1. è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­é…ç½®äº†ç›¸åº”çš„ API å¯†é’¥")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. ç¡®è®¤ API å¯†é’¥æœ‰è¶³å¤Ÿçš„ä½™é¢å’Œæƒé™")
        print("4. æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
    elif successful_tests < total_tests:
        print(f"\nâš ï¸  æœ‰ {total_tests - successful_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®")
    else:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸå®Œæˆï¼")

if __name__ == "__main__":
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = Logger("logs", "swe_bench_processor")
    logger.info("å¼€å§‹ SWE-bench æ•°æ®å¤„ç†å’Œ LLM API æµ‹è¯•ç¨‹åº")
    
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        config = load_config(os.getenv("CONFIG_PATH", "config/config.yaml"))
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶")
        
        # æ£€æŸ¥è¿è¡Œæ¨¡å¼
        run_mode = os.getenv("RUN_MODE", "embedding").lower()  # é»˜è®¤è¿è¡Œæ•°æ®å¤„ç†æ¨¡å¼
        
        if run_mode == "data":
            logger.info("è¿è¡Œæ¨¡å¼: SWE-bench æ•°æ®å¤„ç†")
            # å¤„ç† SWE-bench æ•°æ®é›†
            swe_loader, result = load_swe_bench_data(config, logger)
        elif run_mode == "llm":
            logger.info("è¿è¡Œæ¨¡å¼: LLM API æµ‹è¯•")
            # è¿è¡Œ LLM æµ‹è¯•
            test_provider_models(config, logger)
        elif run_mode == "embedding":
            logger.info("è¿è¡Œæ¨¡å¼: Embedding åŠŸèƒ½æµ‹è¯•")
            # è¿è¡Œ Embedding æµ‹è¯•
            test_embedding_functionality(config, logger)
        elif run_mode == "image_builder":
            logger.info("è¿è¡Œæ¨¡å¼: SWE-bench Image Builder æµ‹è¯•")
            # è¿è¡Œ Image Builder æµ‹è¯•
            # ä»ç¯å¢ƒå˜é‡è·å–æµ‹è¯•å‚æ•°
            test_instance_ids = os.getenv("TEST_INSTANCE_IDS", "django__django-10914,astropy__astropy-12907").split(",")
            dataset_name = os.getenv("TEST_DATASET_NAME", "SWE-bench/SWE-bench_Lite")
            split = os.getenv("TEST_SPLIT", "test")
            max_workers = calculate_optimal_max_workers()
            
            force_rebuild = os.getenv("TEST_FORCE_REBUILD", "false").lower() == "true"
            timeout = int(os.getenv("TEST_TIMEOUT", "600"))
            
            results = test_swe_image_builder(
                instance_ids=test_instance_ids,
                dataset_name=dataset_name,
                split=split,
                max_workers=max_workers,
                force_rebuild=force_rebuild,
                timeout=timeout,
            )
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            total_instances = len(test_instance_ids)
            successful_instances = len([r for r in results["instance_results"].values() if r.get("success", False)])
            print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ: {successful_instances}/{total_instances} æˆåŠŸ")
        elif run_mode == "both":
            logger.info("è¿è¡Œæ¨¡å¼: æ•°æ®å¤„ç† + LLM æµ‹è¯•")
            # å…ˆå¤„ç†æ•°æ®
            swe_loader, result = load_swe_bench_data(config, logger)
            # å†è¿è¡Œ LLM æµ‹è¯•
            test_provider_models(config, logger)
        elif run_mode == "all":
            logger.info("è¿è¡Œæ¨¡å¼: å…¨éƒ¨åŠŸèƒ½æµ‹è¯•")
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            swe_loader, result = load_swe_bench_data(config, logger)
            test_provider_models(config, logger)
            test_embedding_functionality(config, logger)
            test_instance_ids = os.getenv("TEST_INSTANCE_IDS", "django__django-10914").split(",")
            max_workers = calculate_optimal_max_workers()
            results = test_swe_image_builder(
                instance_ids=test_instance_ids,
                dataset_name=os.getenv("TEST_DATASET_NAME", "SWE-bench/SWE-bench_Lite"),
                split=os.getenv("TEST_SPLIT", "test"),
                max_workers=max_workers,
                force_rebuild=os.getenv("TEST_FORCE_REBUILD", "false").lower() == "true",
                timeout=int(os.getenv("TEST_TIMEOUT", "600")),
            )
        else:
            logger.warning(f"æœªçŸ¥è¿è¡Œæ¨¡å¼: {run_mode}ï¼Œé»˜è®¤è¿è¡Œæ•°æ®å¤„ç†")
            swe_loader, result = load_swe_bench_data(config, logger)
        
    except FileNotFoundError as e:
        logger.error(f"é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿åœ¨ src/config/ ç›®å½•ä¸‹æœ‰ config.yaml æ–‡ä»¶")
    except yaml.YAMLError as e:
        logger.error(f"YAML è§£æé”™è¯¯: {e}")
        print(f"âŒ YAML è§£æé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ config.yaml æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
    finally:
        logger.info("SWE-bench å¤„ç†ç¨‹åºç»“æŸ")
        logger.close()