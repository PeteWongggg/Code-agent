"""
LLM API æµ‹è¯•å·¥å…·

æ”¯æŒæµ‹è¯•å¤šç§ LLM å®¢æˆ·ç«¯ï¼š
- OpenAI: GPT ç³»åˆ—æ¨¡å‹
- Anthropic: Claude ç³»åˆ—æ¨¡å‹
- DeepSeek: ä»£ç å’Œæ¨ç†æ¨¡å‹
- OpenRouter: å¤šæ¨¡å‹èšåˆæœåŠ¡
- Private: ç§æœ‰åŒ–éƒ¨ç½²æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python3 src/managers/llm_api/api_tests.py
"""

import asyncio
import json
import sys
import os
from typing import List
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = os.path.join(project_root, '.env')
if os.path.exists(env_path):
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
else:
    print(f"âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")
    print("è¯·å¤åˆ¶ env.example ä¸º .env å¹¶é…ç½®ä½ çš„ API å¯†é’¥")

from src.managers.llm_api.clients.openai.openai_client import OpenAIClient
from src.managers.llm_api.clients.anthropic.anthropic_client import AnthropicClient
from src.managers.llm_api.clients.deepseek.deepseek_client import DeepSeekClient
from src.managers.llm_api.clients.openrouter.openrouter_client import OpenRouterClient
from src.managers.llm_api.clients.private.private_client import PrivateModelClient
from src.managers.llm_api.base_client import ChatMessage, MessageRole, ChatCompletionRequest

# =============================================================================
# é…ç½®éƒ¨åˆ† - åœ¨è¿™é‡Œè®¾ç½®ä½ çš„ API å¯†é’¥å’Œæ¨¡å‹é€‰æ‹©
# =============================================================================

# API å¯†é’¥é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
API_KEYS = {
    "openai": os.getenv("OPENAI_API_KEY", "your-openai-api-key-here"),
    "anthropic": os.getenv("ANTHROPIC_API_KEY", "your-anthropic-api-key-here"),
    "deepseek": os.getenv("DEEPSEEK_API_KEY", "your-deepseek-api-key-here"), 
    "openrouter": os.getenv("OPENROUTER_API_KEY", "your-openrouter-api-key-here"),
    "private": os.getenv("PRIVATE_API_KEY", "EMPTY")
}

# åŸºç¡€ URL é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
BASE_URLS = {
    "openai": os.getenv("OPENAI_BASE_URL"),  # é»˜è®¤ https://api.openai.com/v1
    "anthropic": os.getenv("ANTHROPIC_BASE_URL"),  # é»˜è®¤ https://api.anthropic.com
    "deepseek": os.getenv("DEEPSEEK_BASE_URL"),  # é»˜è®¤ https://api.deepseek.com/v1
    "openrouter": os.getenv("OPENROUTER_BASE_URL"),  # é»˜è®¤ https://openrouter.ai/api/v1
    "private": os.getenv("PRIVATE_URL", "http://localhost:8000/v1")  # ä¿®æ­£ä¸º PRIVATE_URL
}

# ç‰¹æ®Šé…ç½®å‚æ•°
SPECIAL_CONFIGS = {
    "openrouter": {
        "app_name": os.getenv("OPENROUTER_APP_NAME", "tokfinity-llm-client"),
        "site_url": os.getenv("OPENROUTER_SITE_URL", "https://github.com/your-repo")
    },
    "private": {
        "deployment_type": os.getenv("PRIVATE_DEPLOYMENT_TYPE", "vllm"),
        "model_name": os.getenv("PRIVATE_MODEL_NAME", "your-private-model")
    }
}

# å¯ç”¨çš„å®¢æˆ·ç«¯é…ç½®
AVAILABLE_CLIENTS = {
    "openai": {
        "name": "OpenAI",
        "description": "OpenAI å®˜æ–¹ APIï¼Œæ”¯æŒ GPT ç³»åˆ—æ¨¡å‹",
        "client_class": OpenAIClient,
        "models": [
            "gpt-4-turbo-preview",
            "gpt-4",
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k",
        ],
        "default_model": "gpt-3.5-turbo"
    },
    "anthropic": {
        "name": "Anthropic Claude",
        "description": "Anthropic çš„ Claude ç³»åˆ—æ¨¡å‹",
        "client_class": AnthropicClient,
        "models": [
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229", 
            "claude-3-haiku-20240307",
            "claude-2.1",
            "claude-2.0"
        ],
        "default_model": "claude-3-haiku-20240307"
    },
    "deepseek": {
        "name": "DeepSeek",
        "description": "DeepSeek ç³»åˆ—æ¨¡å‹ï¼Œä¸“æ³¨äºä»£ç å’Œæ¨ç†",
        "client_class": DeepSeekClient,
        "models": [
            "deepseek-chat",
            "deepseek-coder",
            "deepseek-math"
        ],
        "default_model": "deepseek-chat"
    },
    "openrouter": {
        "name": "OpenRouter",
        "description": "å¤šæ¨¡å‹èšåˆæœåŠ¡ï¼Œæ”¯æŒå¤šå®¶å‚å•†çš„æ¨¡å‹",
        "client_class": OpenRouterClient,
        "models": [
            "openai/gpt-4-turbo-preview",
            "openai/gpt-3.5-turbo",
            "anthropic/claude-3-opus",
            "anthropic/claude-3-sonnet",
            "anthropic/claude-3-haiku",
            "google/gemini-pro",
            "meta-llama/llama-2-70b-chat",
            "mistralai/mixtral-8x7b-instruct"
        ],
        "default_model": "openai/gpt-3.5-turbo"
    },
    "private": {
        "name": "ç§æœ‰åŒ–éƒ¨ç½²",
        "description": "ç§æœ‰åŒ–éƒ¨ç½²çš„æ¨¡å‹ï¼ˆvLLMã€TGIã€Ollamaç­‰ï¼‰",
        "client_class": PrivateModelClient,
        "models": [
            os.getenv("PRIVATE_MODEL_NAME", "your-custom-model"),
            "llama-2-7b-chat",
            "qwen-2.5-coder",
            "deepseek-coder",
            "codellama"
        ],
        "default_model": os.getenv("PRIVATE_MODEL_NAME", "qwen-2.5-coder")
    }
}






def create_client(client_type: str, model: str = None, api_key: str = None):
    """
    åˆ›å»ºæŒ‡å®šç±»å‹çš„å®¢æˆ·ç«¯
    
    Args:
        client_type: å®¢æˆ·ç«¯ç±»å‹ (openai, anthropic, deepseek, openrouter, private)
        model: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
        api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å¯†é’¥
    
    Returns:
        å®¢æˆ·ç«¯å®ä¾‹
    """
    if client_type not in AVAILABLE_CLIENTS:
        raise ValueError(f"ä¸æ”¯æŒçš„å®¢æˆ·ç«¯ç±»å‹: {client_type}")
    
    config = AVAILABLE_CLIENTS[client_type]
    
    # ä½¿ç”¨ä¼ å…¥çš„APIå¯†é’¥æˆ–é…ç½®ä¸­çš„å¯†é’¥
    if api_key is None:
        api_key = API_KEYS.get(client_type)
    
    base_url = BASE_URLS.get(client_type)
    
    if not api_key or api_key.startswith('your-'):
        print(f"âš ï¸  è­¦å‘Š: {config['name']} çš„ API å¯†é’¥æœªé…ç½®ï¼Œè¯·æ±‚å¯èƒ½ä¼šå¤±è´¥")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client_kwargs = {
        "api_key": api_key,
        "timeout": 30
    }
    
    if base_url:
        client_kwargs["base_url"] = base_url
    
    # ç‰¹æ®Šå¤„ç†ä¸åŒå®¢æˆ·ç«¯çš„å‚æ•°
    if client_type == "openrouter":
        openrouter_config = SPECIAL_CONFIGS.get("openrouter", {})
        client_kwargs["app_name"] = openrouter_config.get("app_name", "tokfinity-examples")
        client_kwargs["site_url"] = openrouter_config.get("site_url")
    elif client_type == "private":
        private_config = SPECIAL_CONFIGS.get("private", {})
        client_kwargs["deployment_type"] = private_config.get("deployment_type", "vllm")
        # ç¡®ä¿ç§æœ‰åŒ–éƒ¨ç½²ä½¿ç”¨æ­£ç¡®çš„ base_url
        if not base_url:
            client_kwargs["base_url"] = "http://localhost:8000/v1"
    
    return config["client_class"](**client_kwargs)


def test_client(client_type: str, model: str = None, test_message: str = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚", api_key: str = None, use_stream=False, tools=None, tool_choice=None):
    """
    æµ‹è¯•æŒ‡å®šçš„å®¢æˆ·ç«¯
    
    Args:
        client_type: å®¢æˆ·ç«¯ç±»å‹
        model: æ¨¡å‹åç§°
        test_message: æµ‹è¯•æ¶ˆæ¯
        api_key: APIå¯†é’¥
    """
    config = AVAILABLE_CLIENTS.get(client_type)
    if not config:
        print(f"âŒ ä¸æ”¯æŒçš„å®¢æˆ·ç«¯ç±»å‹: {client_type}")
        return
    
    # ä½¿ç”¨é»˜è®¤æ¨¡å‹æˆ–æŒ‡å®šæ¨¡å‹
    if not model:
        model = config["default_model"]
    elif model not in config["models"]:
        print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ {model} ä¸åœ¨æ¨èåˆ—è¡¨ä¸­ï¼Œä½†ä»ä¼šå°è¯•ä½¿ç”¨")
    
    print(f"\nğŸ§ª æµ‹è¯• {config['name']} - {model}")
    print("-" * 60)
    
    # ç§æœ‰åŒ–éƒ¨ç½²çš„ç‰¹æ®Šå¤„ç†
    if client_type == "private":
        private_url = os.getenv("PRIVATE_URL", "http://localhost:8000/v1")
        deployment_type = os.getenv("PRIVATE_DEPLOYMENT_TYPE", "vllm")
        print(f"ğŸ  ç§æœ‰åŒ–éƒ¨ç½²ä¿¡æ¯:")
        print(f"   æœåŠ¡åœ°å€: {private_url}")
        print(f"   éƒ¨ç½²ç±»å‹: {deployment_type}")
        print(f"   æ¨¡å‹åç§°: {model}")
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = create_client(client_type, model, api_key)
        print(f"âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ¶ˆæ¯
        messages = [
            ChatMessage(role=MessageRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹,éœ€è¦é€šè¿‡æœç´¢å·¥å…·å¸®æˆ‘è§£å†³é—®é¢˜ã€‚"),
            ChatMessage(role=MessageRole.USER, content=test_message)
        ]
        
        # åˆ›å»ºè¯·æ±‚
        request = client.create_request(
            messages=messages,
            model=model,
            temperature=0.7,
            max_tokens=200,
            stream=use_stream,
            tools=tools,
            #tool_choice=tool_choice
        )
        
        print(f"ğŸ“¤ å‘é€è¯·æ±‚...{request}")
        
        # å‘é€è¯·æ±‚
        response = client.chat_completions_create(request)
        
        # æ˜¾ç¤ºç»“æœ
        if use_stream:
            print(f"âœ… æ”¶åˆ°æµå¼å“åº”ï¼Œå¼€å§‹å¤„ç†...")
            content_parts = []
            chunk_count = 0
            
            for chunk in response:
                chunk_count += 1
                if chunk and chunk.choices:
                    delta = chunk.choices[0].get('delta', {})
                    if 'content' in delta and delta['content']:
                        content_parts.append(delta['content'])
            
            full_content = ''.join(content_parts)
            print(f"âœ… æµå¼å“åº”å®Œæˆ:")
            print(f"   æ€»å—æ•°: {chunk_count}")
            print(f"   å®Œæ•´å†…å®¹: {full_content}")
        else:
            print(f"âœ… æ”¶åˆ°å“åº”: ")
            print(f"   æ¨¡å‹: {response.model}")
            msg = response.choices[0].message
            # æ‰“å°å·¥å…·è°ƒç”¨æˆ–æ–‡æœ¬å†…å®¹
            if getattr(msg, 'tool_calls', None):
                print(f"   æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ tool_calls: {msg.tool_calls}")
            print(f"   å†…å®¹: {msg.content}")
            
            if response.usage:
                print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens} "
                      f"(è¾“å…¥: {response.usage.prompt_tokens}, "
                      f"è¾“å‡º: {response.usage.completion_tokens})")
        
        return True
        
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {error_msg}")
        
        # ç‰¹æ®Šå¤„ç† 502 é”™è¯¯
        if "502" in error_msg or "Bad Gateway" in error_msg:
            print("\nğŸ” 502 Bad Gateway é”™è¯¯è¯Šæ–­:")
            print("ğŸ“‹ å¯èƒ½åŸå› :")
            print("   1. vLLM æœåŠ¡æ­£åœ¨å¯åŠ¨ä¸­ - æ¨¡å‹åŠ è½½éœ€è¦æ—¶é—´ï¼ˆé€šå¸¸å‡ åˆ†é’Ÿï¼‰")
            print("   2. GPU å†…å­˜ä¸è¶³ - æ¨¡å‹å¤ªå¤§æ— æ³•åŠ è½½åˆ° GPU")
            print("   3. vLLM æœåŠ¡å´©æºƒ - æ£€æŸ¥æœåŠ¡æ—¥å¿—")
            print("   4. ä»£ç†æœåŠ¡å™¨é—®é¢˜ - å¦‚æœä½¿ç”¨äº† nginx ç­‰ä»£ç†")
            print("   5. ç«¯å£å†²çªæˆ–ç½‘ç»œé—®é¢˜")
            print("\nğŸ› ï¸  å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("   1. ç­‰å¾… 3-5 åˆ†é’Ÿåé‡è¯•ï¼ˆæ¨¡å‹åŠ è½½æ—¶é—´ï¼‰")
            print("   2. æ£€æŸ¥ vLLM æœåŠ¡æ—¥å¿—: docker logs <container_name>")
            print("   3. æ£€æŸ¥ GPU å†…å­˜ä½¿ç”¨: nvidia-smi")
            print("   4. é‡å¯ vLLM æœåŠ¡")
            print("   5. ç¡®è®¤æ¨¡å‹è·¯å¾„å’Œé…ç½®æ­£ç¡®")
            
        return False
    
    finally:
        try:
            client.close()
        except:
            pass


async def test_client_async(client_type: str, model: str = None, test_message: str = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚", api_key: str = None):
    """
    å¼‚æ­¥æµ‹è¯•æŒ‡å®šçš„å®¢æˆ·ç«¯
    
    Args:
        client_type: å®¢æˆ·ç«¯ç±»å‹
        model: æ¨¡å‹åç§°
        test_message: æµ‹è¯•æ¶ˆæ¯
        api_key: APIå¯†é’¥
    """
    config = AVAILABLE_CLIENTS.get(client_type)
    if not config:
        print(f"âŒ ä¸æ”¯æŒçš„å®¢æˆ·ç«¯ç±»å‹: {client_type}")
        return
    
    if not model:
        model = config["default_model"]
    
    print(f"\nğŸš€ å¼‚æ­¥æµ‹è¯• {config['name']} - {model}")
    print("-" * 60)
    
    # ç§æœ‰åŒ–éƒ¨ç½²çš„ç‰¹æ®Šå¤„ç†
    if client_type == "private":
        private_url = os.getenv("PRIVATE_URL", "http://localhost:8000/v1")
        deployment_type = os.getenv("PRIVATE_DEPLOYMENT_TYPE", "vllm")
        print(f"ğŸ  ç§æœ‰åŒ–éƒ¨ç½²ä¿¡æ¯:")
        print(f"   æœåŠ¡åœ°å€: {private_url}")
        print(f"   éƒ¨ç½²ç±»å‹: {deployment_type}")
        print(f"   æ¨¡å‹åç§°: {model}")
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = create_client(client_type, model, api_key)
        
        # åˆ›å»ºæ¶ˆæ¯
        messages = [
            ChatMessage(role=MessageRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"),
            ChatMessage(role=MessageRole.USER, content=test_message)
        ]
        
        # åˆ›å»ºè¯·æ±‚
        request = client.create_request(
            messages=messages,
            model=model,
            temperature=0.7,
            max_tokens=150
        )
        
        print(f"ğŸ“¤ å‘é€å¼‚æ­¥è¯·æ±‚...")
        
        # å‘é€å¼‚æ­¥è¯·æ±‚
        response = await client.achat_completions_create(request)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"âœ… æ”¶åˆ°å¼‚æ­¥å“åº”:")
        print(f"   æ¨¡å‹: {response.model}")
        print(f"   å†…å®¹: {response.choices[0].message.content}")
        
        if response.usage:
            print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    finally:
        try:
            await client.aclose()
        except:
            pass


def  main(client=None, model=None, api=None, test_clients=None, test_models=None, test_message=None, run_async=False, use_stream=False):
    """
    ä¸»æµ‹è¯•å‡½æ•°
    
    Args:
        client: å•ä¸ªå®¢æˆ·ç«¯ç±»å‹ï¼Œä¾‹å¦‚ "openai"
        model: å•ä¸ªæ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "gpt-4o"
        api: å•ä¸ªAPIå¯†é’¥ï¼Œä¾‹å¦‚ "sk-xxx"
        test_clients: è¦æµ‹è¯•çš„å®¢æˆ·ç«¯åˆ—è¡¨ï¼Œä¾‹å¦‚ ["openai", "deepseek"]
        test_models: è¦æµ‹è¯•çš„æ¨¡å‹å­—å…¸ï¼Œä¾‹å¦‚ {"openai": "gpt-4", "deepseek": "deepseek-chat"}
        test_message: è‡ªå®šä¹‰æµ‹è¯•æ¶ˆæ¯
        run_async: æ˜¯å¦è¿è¡Œå¼‚æ­¥æµ‹è¯•
    
    Examples:
        # ç®€å•æ¨¡å¼ï¼šæµ‹è¯•å•ä¸ªå®¢æˆ·ç«¯
        main(client="openai", model="gpt-4o", api="sk-xxx")
        
        # æ‰¹é‡æ¨¡å¼ï¼šæµ‹è¯•å¤šä¸ªå®¢æˆ·ç«¯
        main(test_clients=["openai", "deepseek"])
        
        # æµ‹è¯•ç‰¹å®šæ¨¡å‹
        main(test_models={"openai": "gpt-4", "deepseek": "deepseek-coder"})
        
        # è‡ªå®šä¹‰æµ‹è¯•æ¶ˆæ¯
        main(client="openai", test_message="è¯·å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—")
    """
    print("ğŸš€ LLM API å®¢æˆ·ç«¯æµ‹è¯•å·¥å…·")
    print("=" * 80)
    
    # å¤„ç†å•ä¸ªå®¢æˆ·ç«¯æµ‹è¯•æ¨¡å¼
    if client and model and api:
        print(f"\nğŸ¯ å•å®¢æˆ·ç«¯æµ‹è¯•æ¨¡å¼")
        print(f"   å®¢æˆ·ç«¯: {client}")
        print(f"   æ¨¡å‹: {model}")
        print(f"   APIå¯†é’¥: {'âœ… å·²æä¾›' if api and not api.startswith('your-') else 'âŒ æ— æ•ˆ'}")
        
        # ä¸´æ—¶æ›´æ–°APIå¯†é’¥
        original_api = API_KEYS.get(client)
        API_KEYS[client] = api
        
        try:
            # æµ‹è¯•å•ä¸ªå®¢æˆ·ç«¯
            if not test_message:
                test_message = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œå¹¶è¯´æ˜ä½ çš„ä¸»è¦åŠŸèƒ½ã€‚"
            
            print(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_message}")
            print("\n" + "=" * 80)
            
            if run_async:
                success = asyncio.run(test_client_async(client, model, test_message, api))
            else:
                success = test_client(client, model, test_message, api, use_stream)
            
            print("\n" + "=" * 80)
            if success:
                print("ğŸ‰ å•å®¢æˆ·ç«¯æµ‹è¯•æˆåŠŸï¼")
            else:
                print("âŒ å•å®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥ï¼")
            
        finally:
            # æ¢å¤åŸå§‹APIå¯†é’¥
            if original_api:
                API_KEYS[client] = original_api
        
        return
    
    # ç¡®å®šè¦æµ‹è¯•çš„å®¢æˆ·ç«¯ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
    if test_clients:
        clients_to_test = test_clients
    elif test_models:
        clients_to_test = list(test_models.keys())
    else:
        # é»˜è®¤æµ‹è¯•æ‰€æœ‰æœ‰APIå¯†é’¥çš„å®¢æˆ·ç«¯
        clients_to_test = []
        for client_id, api_key in API_KEYS.items():
            if api_key and not api_key.startswith('your-'):
                clients_to_test.append(client_id)
        
        if not clients_to_test:
            print("\nâš ï¸  æ²¡æœ‰é…ç½®æœ‰æ•ˆçš„APIå¯†é’¥ï¼Œå°†æµ‹è¯•æ‰€æœ‰å®¢æˆ·ç«¯ï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼‰")
            clients_to_test = list(AVAILABLE_CLIENTS.keys())
    
    print(f"\nğŸ¯ å°†æµ‹è¯•ä»¥ä¸‹å®¢æˆ·ç«¯: {', '.join(clients_to_test)}")
    
    # é»˜è®¤æµ‹è¯•æ¶ˆæ¯
    if not test_message:
        test_message = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œå¹¶è¯´æ˜ä½ çš„ä¸»è¦åŠŸèƒ½ã€‚"
    
    print(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_message}")
    print("\n" + "=" * 80)
    
    # è¿è¡Œæµ‹è¯•
    success_count = 0
    total_count = len(clients_to_test)
    
    for client_type in clients_to_test:
        if client_type not in AVAILABLE_CLIENTS:
            print(f"âŒ è·³è¿‡æœªçŸ¥å®¢æˆ·ç«¯: {client_type}")
            continue
        
        # ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹
        model = None
        if test_models and client_type in test_models:
            model = test_models[client_type]
        
        # è¿è¡Œæµ‹è¯•
        if run_async:
            success = asyncio.run(test_client_async(client_type, model, test_message))
        else:
            success = test_client(client_type, model, test_message)
        
        if success:
            success_count += 1
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "=" * 80)
    print(f"ğŸ æµ‹è¯•å®Œæˆ: {success_count}/{total_count} ä¸ªå®¢æˆ·ç«¯æµ‹è¯•æˆåŠŸ")
    
    if success_count == 0:
        print("\nğŸ’¡ æç¤º:")
        print("1. è¯·ç¡®ä¿åœ¨æ–‡ä»¶é¡¶éƒ¨çš„ API_KEYS ä¸­é…ç½®äº†æœ‰æ•ˆçš„APIå¯†é’¥")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. ç¡®è®¤APIå¯†é’¥æœ‰è¶³å¤Ÿçš„ä½™é¢å’Œæƒé™")
    elif success_count < total_count:
        print(f"\nâš ï¸  æœ‰ {total_count - success_count} ä¸ªå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    else:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸå®Œæˆï¼")


if __name__ == "__main__":
    # =============================================================================
    # ğŸ”§ é…ç½®åŒºåŸŸ - ä»ç¯å¢ƒå˜é‡å’Œé¢„å®šä¹‰åˆ—è¡¨ä¸­è·å–é…ç½®
    # =============================================================================
    
    # æ¨¡å‹æ˜ å°„ï¼ˆæœ€æ–°çš„æ¨¡å‹åˆ—è¡¨ï¼‰
    model_map = {
        "openai": ["gpt-4o", "gpt-4o-mini"],
        "anthropic": ["claude-3-5-sonnet-20241022", "claude-3-haiku-20240307"],
        "deepseek": ["deepseek-chat", "deepseek-reasoner"],
        "openrouter": ["openai/gpt-4o", "anthropic/claude-3-5-sonnet"],
        "private": [os.getenv("PRIVATE_MODEL_NAME", "qwen-2.5-coder")]
    }

    # API å¯†é’¥æ˜ å°„ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    api_map = {
        "openai": os.getenv("OPENAI_API_KEY", "your-openai-api-key-here"),
        "anthropic": os.getenv("ANTHROPIC_API_KEY", "your-anthropic-api-key-here"),
        "deepseek": os.getenv("DEEPSEEK_API_KEY", "your-deepseek-api-key-here"),
        "openrouter": os.getenv("OPENROUTER_API_KEY", "your-openrouter-api-key-here"),
        "private": os.getenv("PRIVATE_API_KEY", "EMPTY")
    }

    # å®¢æˆ·ç«¯åˆ—è¡¨
    client_list = ["openai", "anthropic", "deepseek", "openrouter", "private"]
    
    # =============================================================================
    # ğŸ¯ æµ‹è¯•é…ç½® - ä¿®æ”¹è¿™é‡Œæ¥é€‰æ‹©è¦æµ‹è¯•çš„å®¢æˆ·ç«¯å’Œæ¨¡å‹
    # =============================================================================
    
    client_2_use = "private"           # ä½¿ç”¨ OpenAI åšå‡½æ•°è°ƒç”¨æµ‹è¯•
    api_key = api_map[client_2_use]         # è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    model = model_map[client_2_use][0] # é€‰æ‹©æ¨¡å‹
    use_stream=False # å·¥å…·è°ƒç”¨å»ºè®®ä½¿ç”¨éæµå¼ï¼Œä¾¿äºè§‚å¯Ÿ tool_calls
    
    # =============================================================================
    # ğŸ¯ ä¸»è¦æµ‹è¯•åŒºåŸŸ - ä¿®æ”¹ä¸‹é¢çš„å‚æ•°æ¥æµ‹è¯•ä¸åŒçš„é…ç½®
    # =============================================================================
    
    # æ˜¾ç¤ºç¯å¢ƒå˜é‡åŠ è½½çŠ¶æ€
    print(f"\nğŸ” ç¯å¢ƒå˜é‡åŠ è½½çŠ¶æ€:")
    for client_id, api_key_value in api_map.items():
        status = "âœ… å·²é…ç½®" if api_key_value and not api_key_value.startswith('your-') else "âŒ éœ€è¦é…ç½®"
        print(f"   {client_id.upper()}_API_KEY: {status}")
    
    
    # å½“å‰æµ‹è¯•é…ç½®
    print(f"\nğŸ® å½“å‰æµ‹è¯•é…ç½®:")
    print(f"   å®¢æˆ·ç«¯: {client_2_use}")
    print(f"   æ¨¡å‹: {model}")
    print(f"   å¯ç”¨æ¨¡å‹: {model_map[client_2_use]}")
    print(f"   æ˜¯å¦å¯ç”¨æµå¼: {use_stream}")
    print(f"   APIçŠ¶æ€: {'âœ… å·²é…ç½®' if api_key and not api_key.startswith('your-') else 'âŒ éœ€è¦é…ç½®'}")
    
    if not api_key or api_key.startswith('your-'):
        print(f"\nâš ï¸  æç¤º: è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® {client_2_use.upper()}_API_KEY")
        print(f"   1. å¤åˆ¶ env.example ä¸º .env")
        print(f"   2. ç¼–è¾‘ .env æ–‡ä»¶ä¸­çš„ {client_2_use.upper()}_API_KEY")
    
    # å‡†å¤‡å¸¦ tools çš„æµ‹è¯•
    tools = [
        {
            "type": "function",
            "function": {
                "name": "web_search",
                "description": "è”ç½‘æœç´¢æœ€æ–°çš„äº‹å®ä¿¡æ¯ï¼Œå¹¶è¿”å›ç®€è¦æ‘˜è¦ä¸å…³é”®æ¥æºé“¾æ¥ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "è¦æœç´¢çš„æŸ¥è¯¢è¯­å¥"},
                        "time_range": {"type": "string", "enum": ["day","week","month","year","all"], "description": "æ—¶é—´èŒƒå›´"}
                    },
                    "required": ["query"]
                }
            }
        }
    ]

    fed_question = "ç¾è”å‚¨æœ€æ–°çš„æ–°é—»æœ‰å•¥ï¼Ÿä½ éœ€è¦é€šè¿‡è”ç½‘æœç´¢åå›ç­”æˆ‘ã€‚"

    # æ‰§è¡Œæµ‹è¯•ï¼ˆç§æœ‰ vLLM å¯èƒ½ä¸æ”¯æŒ OpenAI-style toolsï¼Œå…ˆå°è¯•ç¦ç”¨ tools ä»¥å®šä½ 400ï¼‰
    tools_to_use = tools
    tool_choice_to_use = "auto"

    test_client(
        client_type=client_2_use,
        model=model,
        test_message=fed_question,
        api_key=api_key,
        use_stream=use_stream,
        tools=tools_to_use,
        tool_choice=tool_choice_to_use
    )
    
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ å¿«é€Ÿä½¿ç”¨æŒ‡å—:")
    print("1. ç¯å¢ƒé…ç½®:")
    print("   - å¤åˆ¶ env.example ä¸º .env")
    print("   - åœ¨ .env æ–‡ä»¶ä¸­é…ç½®ä½ çš„APIå¯†é’¥")
    print("2. å®¢æˆ·ç«¯é€‰æ‹©:")
    print("   - client_2_use = client_list[0-4] æ¥åˆ‡æ¢å®¢æˆ·ç«¯")
    print("   - 0:OpenAI, 1:Anthropic, 2:DeepSeek, 3:OpenRouter, 4:Private")
    print("3. æ¨¡å‹é€‰æ‹©:")
    print("   - model = model_map[client_2_use][0-1] æ¥åˆ‡æ¢æ¨¡å‹")
    print("4. ç§æœ‰åŒ–éƒ¨ç½²é…ç½®:")
    print("   - PRIVATE_URL: ç§æœ‰åŒ–æœåŠ¡åœ°å€ (å¦‚: https://127.0.0.1:33/v1)")
    print("   - PRIVATE_MODEL_NAME: æ¨¡å‹åç§° (å¦‚: qwen-2.5-coder)")
    print("   - PRIVATE_DEPLOYMENT_TYPE: éƒ¨ç½²ç±»å‹ (vllm/tgi/ollama)")
    print("   - PRIVATE_API_KEY: APIå¯†é’¥ (é€šå¸¸ä¸º EMPTY)")
    print("5. é«˜çº§é€‰é¡¹:")
    print("   - å–æ¶ˆæ³¨é‡Šä¸Šé¢çš„é€‰é¡¹æ¥æµ‹è¯•æ›´å¤šåŠŸèƒ½")
    print("=" * 80)
